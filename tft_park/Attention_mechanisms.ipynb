{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(q, k, v, mask):\n",
    "  print(\"Applying SELF ATTENTION:\")\n",
    "  print(\"-------------------------------------------------\")\n",
    "  \n",
    "  matmul_qk = tf.matmul(q, k, transpose_b=True) \n",
    "  print(\"\\nmultiplying QUERIE and KEY:\")\n",
    "  print(matmul_qk)\n",
    "\n",
    "  dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "  print(\"\\nTaking the dk size:\")\n",
    "  print(dk)\n",
    "\n",
    "  scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "  print(\"\\nStandarding the process:\")\n",
    "  print(scaled_attention_logits)\n",
    "\n",
    "  if mask is not None:\n",
    "    scaled_attention_logits += (mask * -1e9)\n",
    "\n",
    "  attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  \n",
    "  print(\"\\nApplying SOFTMAX in the Standarded vlaues, CALCULATING attention weights:\")\n",
    "  print(attention_weights)\n",
    "  output = tf.matmul(attention_weights, v)\n",
    "  print(\"\\nCalculating the outpu, multipling the attention weights and the VALUE MATRIX:\")\n",
    "  print(output)\n",
    "  print(\"-------------------------------------------------\")\n",
    "  return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 6, 2), dtype=float32, numpy=\n",
       "array([[[0.2251054 , 0.02708256],\n",
       "        [0.30044246, 0.4288795 ],\n",
       "        [0.3827877 , 0.9703628 ],\n",
       "        [0.44823313, 0.7915255 ],\n",
       "        [0.51326764, 0.28244972],\n",
       "        [0.36063552, 0.5849689 ]]], dtype=float32)>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = tf.random.uniform((1, 6, 2))  # (batch_size, encoder_sequence, d_model)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying SELF ATTENTION:\n",
      "-------------------------------------------------\n",
      "\n",
      "multiplying QUERIE and KEY:\n",
      "tf.Tensor(\n",
      "[[[0.50541544 0.9106476  0.5522354  0.458029   0.07258042 0.58931434]\n",
      "  [0.9106476  1.676973   1.0908767  0.74042755 0.1297733  1.1660134 ]\n",
      "  [0.5522354  1.0908767  0.8573848  0.2756889  0.07665293 0.9199638 ]\n",
      "  [0.458029   0.74042755 0.2756889  0.613996   0.06812157 0.28976473]\n",
      "  [0.07258042 0.1297733  0.07665293 0.06812157 0.01045062 0.08174735]\n",
      "  [0.58931434 1.1660134  0.9199638  0.28976473 0.08174735 0.9871798 ]]], shape=(1, 6, 6), dtype=float32)\n",
      "\n",
      "Taking the dk size:\n",
      "tf.Tensor(2.0, shape=(), dtype=float32)\n",
      "\n",
      "Standarding the process:\n",
      "tf.Tensor(\n",
      "[[[0.35738268 0.6439251  0.39048943 0.32387543 0.05132211 0.41670817]\n",
      "  [0.6439251  1.185799   0.7713663  0.52356136 0.09176359 0.824496  ]\n",
      "  [0.39048943 0.7713663  0.6062626  0.19494149 0.05420181 0.65051264]\n",
      "  [0.32387543 0.52356136 0.19494149 0.43416077 0.04816922 0.20489462]\n",
      "  [0.05132211 0.09176359 0.05420181 0.04816922 0.0073897  0.0578041 ]\n",
      "  [0.41670817 0.824496   0.65051264 0.20489462 0.0578041  0.69804156]]], shape=(1, 6, 6), dtype=float32)\n",
      "\n",
      "Applying SOFTMAX in the Standarded vlaues, CALCULATING attention weights:\n",
      "tf.Tensor(\n",
      "[[[0.16313252 0.21726227 0.1686237  0.15775695 0.12012132 0.17310327]\n",
      "  [0.15348376 0.26387322 0.17434502 0.1360784  0.08836124 0.18385836]\n",
      "  [0.15294847 0.22385038 0.18978167 0.12578236 0.10926908 0.19836807]\n",
      "  [0.17054634 0.20824036 0.1499157  0.19043148 0.1294508  0.15141527]\n",
      "  [0.16654098 0.17341419 0.16702124 0.16601671 0.15938282 0.167624  ]\n",
      "  [0.15153037 0.22782409 0.19144306 0.12260558 0.10583507 0.20076185]]], shape=(1, 6, 6), dtype=float32)\n",
      "\n",
      "Calculating the outpu, multipling the attention weights and the VALUE MATRIX:\n",
      "tf.Tensor(\n",
      "[[[0.6205936  0.49994856]\n",
      "  [0.6732347  0.51955765]\n",
      "  [0.6620698  0.4869053 ]\n",
      "  [0.58101827 0.511481  ]\n",
      "  [0.5758982  0.4720261 ]\n",
      "  [0.6685867  0.48788267]]], shape=(1, 6, 2), dtype=float32)\n",
      "-------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(1, 6, 2), dtype=float32, numpy=\n",
       " array([[[0.6205936 , 0.49994856],\n",
       "         [0.6732347 , 0.51955765],\n",
       "         [0.6620698 , 0.4869053 ],\n",
       "         [0.58101827, 0.511481  ],\n",
       "         [0.5758982 , 0.4720261 ],\n",
       "         [0.6685867 , 0.48788267]]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(1, 6, 6), dtype=float32, numpy=\n",
       " array([[[0.16313252, 0.21726227, 0.1686237 , 0.15775695, 0.12012132,\n",
       "          0.17310327],\n",
       "         [0.15348376, 0.26387322, 0.17434502, 0.1360784 , 0.08836124,\n",
       "          0.18385836],\n",
       "         [0.15294847, 0.22385038, 0.18978167, 0.12578236, 0.10926908,\n",
       "          0.19836807],\n",
       "         [0.17054634, 0.20824036, 0.1499157 , 0.19043148, 0.1294508 ,\n",
       "          0.15141527],\n",
       "         [0.16654098, 0.17341419, 0.16702124, 0.16601671, 0.15938282,\n",
       "          0.167624  ],\n",
       "         [0.15153037, 0.22782409, 0.19144306, 0.12260558, 0.10583507,\n",
       "          0.20076185]]], dtype=float32)>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled_dot_product_attention(y, y, y, mask=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "  def __init__(self, d_model, num_heads):\n",
    "    super(MultiHeadAttention, self).__init__()\n",
    "    self.num_heads = num_heads\n",
    "    self.d_model = d_model\n",
    "\n",
    "    assert d_model % self.num_heads == 0\n",
    "\n",
    "    self.depth = d_model // self.num_heads\n",
    "\n",
    "    self.wq = tf.keras.layers.Dense(d_model,use_bias=False)\n",
    "    self.wk = tf.keras.layers.Dense(d_model,use_bias=False)\n",
    "    self.wv = tf.keras.layers.Dense(d_model,use_bias=False)\n",
    "\n",
    "    self.dense = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "  def split_heads(self, x, batch_size):\n",
    "    \"\"\"Split the last dimension into (num_heads, depth).\n",
    "    Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
    "    \"\"\"\n",
    "    x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "    return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "  def call(self, v, k, q, mask):\n",
    "    batch_size = tf.shape(q)[0]\n",
    "    print(\"Batch size = :{}\".format(batch_size))\n",
    "    q = self.wq(q)\n",
    "    k = self.wk(k)  \n",
    "    v = self.wv(v)\n",
    "    print(\"Querie : \\n\")  \n",
    "    print(q)\n",
    "    print(\"\\n Key : \\n\")  \n",
    "    print(k)\n",
    "    print(\"\\n Value : \\n\")  \n",
    "    print(v)\n",
    "\n",
    "    q = self.split_heads(q, batch_size) \n",
    "    k = self.split_heads(k, batch_size)  \n",
    "    v = self.split_heads(v, batch_size)\n",
    "\n",
    "    print(\"\\n Querie splited: \\n\")  \n",
    "    print(q)  \n",
    "    print(\"\\n Key splited: \\n\")  \n",
    "    print(k)\n",
    "    print(\"\\n Value splited: \\n\")  \n",
    "    print(v)  \n",
    "\n",
    "    scaled_attention, attention_weights = scaled_dot_product_attention(\n",
    "        q, k, v, mask)\n",
    "\n",
    "    scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3]) \n",
    "    print(\"\\n Transposing the output self attention \\n\")\n",
    "    print(scaled_attention)\n",
    "    concat_attention = tf.reshape(scaled_attention,\n",
    "                                  (batch_size, -1, self.d_model))  \n",
    "    print(\"\\n Concating the Transposed output self attention \\n\")\n",
    "    print(concat_attention)\n",
    "    output = self.dense(concat_attention)  \n",
    "    print(\"\\n Applying into a dense layer the Concat Transposed output self attention \\n\")\n",
    "    print(output)\n",
    "    return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_mha = MultiHeadAttention(d_model=32, num_heads=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size = :Tensor(\"strided_slice:0\", shape=(), dtype=int32)\n",
      "Querie : \n",
      "\n",
      "Tensor(\"dense_1/MatMul:0\", shape=(1, 6, 32), dtype=float32)\n",
      "\n",
      " Key : \n",
      "\n",
      "Tensor(\"dense_1_2/MatMul:0\", shape=(1, 6, 32), dtype=float32)\n",
      "\n",
      " Value : \n",
      "\n",
      "Tensor(\"dense_2_1/MatMul:0\", shape=(1, 6, 32), dtype=float32)\n",
      "\n",
      " Querie splited: \n",
      "\n",
      "Tensor(\"transpose:0\", shape=(1, 2, 6, 16), dtype=float32)\n",
      "\n",
      " Key splited: \n",
      "\n",
      "Tensor(\"transpose_1:0\", shape=(1, 2, 6, 16), dtype=float32)\n",
      "\n",
      " Value splited: \n",
      "\n",
      "Tensor(\"transpose_2:0\", shape=(1, 2, 6, 16), dtype=float32)\n",
      "Applying SELF ATTENTION:\n",
      "-------------------------------------------------\n",
      "\n",
      "multiplying QUERIE and KEY:\n",
      "Tensor(\"MatMul:0\", shape=(1, 2, 6, 6), dtype=float32)\n",
      "\n",
      "Taking the dk size:\n",
      "Tensor(\"Cast:0\", shape=(), dtype=float32)\n",
      "\n",
      "Standarding the process:\n",
      "Tensor(\"truediv:0\", shape=(1, 2, 6, 6), dtype=float32)\n",
      "\n",
      "Applying SOFTMAX in the Standarded vlaues, CALCULATING attention weights:\n",
      "Tensor(\"Softmax:0\", shape=(1, 2, 6, 6), dtype=float32)\n",
      "\n",
      "Calculating the outpu, multipling the attention weights and the VALUE MATRIX:\n",
      "Tensor(\"MatMul_1:0\", shape=(1, 2, 6, 16), dtype=float32)\n",
      "-------------------------------------------------\n",
      "\n",
      " Transposing the output self attention \n",
      "\n",
      "Tensor(\"transpose_3:0\", shape=(1, 6, 2, 16), dtype=float32)\n",
      "\n",
      " Concating the Transposed output self attention \n",
      "\n",
      "Tensor(\"Reshape_3:0\", shape=(1, 6, 32), dtype=float32)\n",
      "\n",
      " Applying into a dense layer the Concat Transposed output self attention \n",
      "\n",
      "Tensor(\"dense_3_1/Add:0\", shape=(1, 6, 32), dtype=float32)\n",
      "Batch size = :1\n",
      "Querie : \n",
      "\n",
      "tf.Tensor(\n",
      "[[[-2.82609463e-01 -2.08682790e-02 -2.31040176e-02  3.51857916e-02\n",
      "    3.03702261e-02  1.22988544e-01  3.15239400e-01 -3.86683613e-01\n",
      "   -8.36746320e-02  2.23775938e-01 -1.87846497e-01  1.48192465e-01\n",
      "    1.59224451e-01 -2.30049044e-01 -2.81861909e-02 -2.27313593e-01\n",
      "   -6.64567500e-02  3.99258256e-01  9.34710428e-02  1.09052934e-01\n",
      "    2.32163887e-03  1.66981947e-02  8.50380734e-02 -1.26079991e-01\n",
      "   -6.93730265e-02 -6.38409331e-02  6.91394359e-02  7.40031973e-02\n",
      "    1.13783553e-01 -2.15226129e-01  1.99367285e-01 -2.64025182e-01]\n",
      "  [-5.42927802e-01 -1.32470816e-01 -9.11547318e-02  3.31168436e-02\n",
      "   -4.67874967e-02  1.78551868e-01  5.77106714e-01 -7.02008009e-01\n",
      "   -6.22923933e-02  3.94008428e-01 -2.82211602e-01  2.00737268e-01\n",
      "    3.14434081e-01 -4.53216732e-01  1.59310475e-02 -4.11998689e-01\n",
      "   -6.53296337e-02  7.31058955e-01  1.88000575e-01  1.83757246e-01\n",
      "   -3.98740992e-02  1.07610293e-01  6.98466599e-02 -2.60234207e-01\n",
      "   -1.29348919e-01 -6.74001426e-02  4.82236110e-02  1.07714690e-01\n",
      "    2.64694631e-01 -3.50822002e-01  4.04061079e-01 -4.53015447e-01]\n",
      "  [-3.98145616e-01 -2.74146616e-01 -1.56456620e-01 -4.17773984e-02\n",
      "   -2.35745743e-01  2.03369800e-02  3.68589848e-01 -4.36517477e-01\n",
      "    1.42963722e-01  2.20167264e-01 -5.62317669e-02 -1.36593347e-02\n",
      "    2.46954888e-01 -3.53939652e-01  1.45957261e-01 -2.54807860e-01\n",
      "    7.15398490e-02  4.67197359e-01  1.54020682e-01  8.54233727e-02\n",
      "   -1.14185870e-01  2.23632157e-01 -1.27968594e-01 -2.25362152e-01\n",
      "   -8.73346776e-02  5.64258136e-02 -1.26734212e-01  1.29750324e-02\n",
      "    2.82441258e-01 -1.37222603e-01  3.36645901e-01 -2.28343800e-01]\n",
      "  [-1.77036971e-01  2.03516260e-01  9.51785520e-02  1.02879956e-01\n",
      "    2.65511930e-01  2.12381452e-01  2.64313847e-01 -3.38028550e-01\n",
      "   -2.83252746e-01  2.24333674e-01 -3.02106440e-01  2.89677769e-01\n",
      "    7.97119439e-02 -1.17702350e-01 -1.81962520e-01 -2.00305104e-01\n",
      "   -1.87794134e-01  3.34432811e-01  3.87866609e-02  1.28679439e-01\n",
      "    1.05397657e-01 -1.66624770e-01  2.72536784e-01 -3.67351659e-02\n",
      "   -5.26607484e-02 -1.69519082e-01  2.41663530e-01  1.27138436e-01\n",
      "   -3.68103459e-02 -2.81720310e-01  7.55342618e-02 -2.92491585e-01]\n",
      "  [-3.96515876e-02 -3.73325136e-04 -1.94830413e-03  5.89021528e-03\n",
      "    7.16834376e-03  1.88521966e-02  4.50180545e-02 -5.53836189e-02\n",
      "   -1.44626312e-02  3.23894881e-02 -2.85311751e-02  2.31139045e-02\n",
      "    2.21037511e-02 -3.19656022e-02 -5.89260878e-03 -3.25762890e-02\n",
      "   -1.10481838e-02  5.70125878e-02  1.28813302e-02  1.60126816e-02\n",
      "    1.55171950e-03  2.54175713e-04  1.45174647e-02 -1.71914045e-02\n",
      "   -9.84194502e-03 -1.04849488e-02  1.20401308e-02  1.13358144e-02\n",
      "    1.46895554e-02 -3.19299363e-02  2.73901261e-02 -3.85431796e-02]\n",
      "  [-4.26641434e-01 -2.97512829e-01 -1.69550508e-01 -4.61652987e-02\n",
      "   -2.56880552e-01  1.94523167e-02  3.93814623e-01 -4.66103137e-01\n",
      "    1.57187387e-01  2.34469816e-01 -5.70671931e-02 -1.80407353e-02\n",
      "    2.64976174e-01 -3.79728287e-01  1.59244746e-01 -2.72043467e-01\n",
      "    7.91874602e-02  4.99177217e-01  1.65385962e-01  9.04934332e-02\n",
      "   -1.24155678e-01  2.42699936e-01 -1.40918970e-01 -2.42222145e-01\n",
      "   -9.34261978e-02  6.27040341e-02 -1.39234617e-01  1.25068454e-02\n",
      "    3.04524988e-01 -1.44503742e-01  3.61593574e-01 -2.42488950e-01]]], shape=(1, 6, 32), dtype=float32)\n",
      "\n",
      " Key : \n",
      "\n",
      "tf.Tensor(\n",
      "[[[-0.19857809 -0.14171208 -0.05419081  0.04152012 -0.1293508\n",
      "   -0.03076985  0.01879204  0.03717022  0.21034594 -0.00192696\n",
      "    0.00660674 -0.16511318 -0.04142038  0.31210625  0.08237282\n",
      "   -0.3880453  -0.11377507 -0.33995554  0.11080513 -0.16393432\n",
      "   -0.0275334  -0.25983188  0.12101447 -0.06567758 -0.2642448\n",
      "    0.3756424  -0.16016002  0.26870462 -0.19108936  0.30910844\n",
      "   -0.00081298 -0.05797506]\n",
      "  [-0.36612555 -0.18367968 -0.06365234  0.0879118  -0.27404824\n",
      "    0.03877728 -0.04131875  0.02470064  0.35424134  0.06689715\n",
      "   -0.06122686 -0.25439733 -0.04940116  0.58509856  0.19324271\n",
      "   -0.70917195 -0.2776431  -0.6023139   0.24635272 -0.33738387\n",
      "   -0.14484933 -0.509746    0.17709115 -0.06379591 -0.49881873\n",
      "    0.6820208  -0.34850967  0.5041742  -0.33220163  0.55365574\n",
      "   -0.06133844 -0.17298432]\n",
      "  [-0.23904665  0.03499711  0.03083392  0.0800772  -0.24992082\n",
      "    0.21599494 -0.17863908 -0.0713793   0.16424507  0.18432641\n",
      "   -0.1865296  -0.06622115  0.02158353  0.40129575  0.20876029\n",
      "   -0.45048487 -0.3167775  -0.34439468  0.24481054 -0.29042023\n",
      "   -0.28240794 -0.39407757  0.02373343  0.07273519 -0.348884\n",
      "    0.42420593 -0.33378905  0.34665617 -0.1767358   0.32902762\n",
      "   -0.15951405 -0.24489449]\n",
      "  [-0.16042645 -0.2964218  -0.12879519  0.00690997 -0.02112875\n",
      "   -0.24878202  0.19328748  0.13279335  0.24866536 -0.16672927\n",
      "    0.177445   -0.25068292 -0.09668784  0.22950178 -0.03044404\n",
      "   -0.3282188   0.06721213 -0.3320229  -0.00908813 -0.05006963\n",
      "    0.1983422  -0.13797067  0.20567799 -0.18739244 -0.18623088\n",
      "    0.32824156 -0.0046205   0.19655642 -0.2015408   0.28784007\n",
      "    0.13963917  0.10812221]\n",
      "  [-0.02828651 -0.0223321  -0.00872196  0.00560021 -0.01744208\n",
      "   -0.00702414  0.00477755  0.0065068   0.0308914  -0.00222265\n",
      "    0.00297106 -0.02490302 -0.00664586  0.04419101  0.01058964\n",
      "   -0.05544891 -0.01432985 -0.04910185  0.01462065 -0.02238015\n",
      "   -0.00132026 -0.03616329  0.01851074 -0.01093988 -0.03731906\n",
      "    0.05380062 -0.02134242  0.03803363 -0.02777606  0.04448063\n",
      "    0.00153895 -0.00643058]\n",
      "  [-0.2555325   0.04109249  0.03468083  0.08613871 -0.2688438\n",
      "    0.23542257 -0.19456327 -0.07838161  0.17397897  0.20038107\n",
      "   -0.20287655 -0.06841448  0.02435152  0.42942935  0.22512026\n",
      "   -0.4812546  -0.34184435 -0.36698472  0.26368934 -0.312116\n",
      "   -0.30634817 -0.42271104  0.02318638  0.08046985 -0.3734962\n",
      "    0.45296007 -0.35933375  0.37097868 -0.18796994  0.35094765\n",
      "   -0.1733541  -0.26491952]]], shape=(1, 6, 32), dtype=float32)\n",
      "\n",
      " Value : \n",
      "\n",
      "tf.Tensor(\n",
      "[[[ 1.92282185e-01  9.74319428e-02  1.43936142e-01 -1.21906511e-01\n",
      "   -1.68756455e-01  1.14047974e-01 -8.63031372e-02  1.13671504e-01\n",
      "   -7.17714429e-02 -1.30521446e-01 -1.88675150e-03  1.63959354e-01\n",
      "   -1.16673067e-01  2.52246350e-01  7.46425688e-02 -1.67255431e-01\n",
      "   -3.23836654e-02 -1.32410690e-01 -6.25965465e-03  4.69313674e-02\n",
      "    2.27472156e-01  1.43550456e-01 -1.21982127e-01  3.17087442e-01\n",
      "   -1.82999209e-01  5.65046631e-02  4.60591651e-02 -1.20136619e-01\n",
      "   -1.46330461e-01  2.44315401e-01 -1.51730791e-01 -2.45779827e-01]\n",
      "  [ 3.88954878e-01  1.87352508e-01  2.65337378e-01 -2.74482906e-01\n",
      "   -2.97206819e-01  2.47903332e-01 -1.01366378e-01  1.29180416e-01\n",
      "   -1.71603069e-01 -1.83090225e-01 -6.85110241e-02  3.12219024e-01\n",
      "   -2.43922070e-01  4.80477840e-01  1.26057982e-01 -3.35862547e-01\n",
      "    4.62244079e-02 -1.62654325e-01 -4.39982414e-02  1.50289282e-01\n",
      "    4.59995925e-01  2.76037842e-01 -2.78529674e-01  5.82496583e-01\n",
      "   -2.90953040e-01  1.69457972e-01  5.34372590e-02 -2.09576145e-01\n",
      "   -2.26239964e-01  4.11163151e-01 -2.83607721e-01 -4.80186343e-01]\n",
      "  [ 3.22703898e-01  1.37724057e-01  1.73155636e-01 -2.78474033e-01\n",
      "   -1.66227624e-01  2.36982062e-01  4.91186157e-02 -7.61698559e-02\n",
      "   -1.90452114e-01 -4.63281851e-03 -1.74564287e-01  2.23657772e-01\n",
      "   -2.16771632e-01  3.44457716e-01  5.92192262e-02 -2.74164915e-01\n",
      "    2.41665184e-01  5.64629920e-02 -9.35252756e-02  2.25418329e-01\n",
      "    3.81385356e-01  2.02924490e-01 -2.88916916e-01  3.76066983e-01\n",
      "   -9.72331539e-02  2.40964442e-01 -2.79654134e-02 -1.13028564e-01\n",
      "   -6.07599169e-02  1.90012336e-01 -1.92868680e-01 -3.67488027e-01]\n",
      "  [ 7.46006370e-02  6.06278144e-02  1.16382971e-01  1.80838667e-02\n",
      "   -1.69006631e-01  3.91409779e-03 -2.05128133e-01  2.80332804e-01\n",
      "    3.41004245e-02 -2.40389273e-01  1.50946528e-01  1.09197997e-01\n",
      "   -2.67165620e-02  1.67672768e-01  8.74122754e-02 -7.06758201e-02\n",
      "   -2.74521887e-01 -2.97994912e-01  7.10398182e-02 -1.11573517e-01\n",
      "    8.85873064e-02  8.93165469e-02  2.71836445e-02  2.61158615e-01\n",
      "   -2.56742448e-01 -1.07398622e-01  1.11024730e-01 -1.25011861e-01\n",
      "   -2.20332578e-01  2.89493233e-01 -1.13538623e-01 -1.35179177e-01]\n",
      "  [ 2.64373887e-02  1.36654107e-02  2.05042046e-02 -1.59901045e-02\n",
      "   -2.44239196e-02  1.52050350e-02 -1.38905458e-02  1.84152760e-02\n",
      "   -9.13741067e-03 -2.01837886e-02  1.52959139e-03  2.30808780e-02\n",
      "   -1.58229005e-02  3.55053805e-02  1.09522389e-02 -2.30646245e-02\n",
      "   -7.54223438e-03 -2.11143419e-02  5.88250350e-06  4.92198020e-03\n",
      "    3.12796868e-02  2.01337002e-02 -1.58928279e-02  4.52264771e-02\n",
      "   -2.73518357e-02  6.24367129e-03  7.43153226e-03 -1.74426306e-02\n",
      "   -2.20485069e-02  3.58880609e-02 -2.15066951e-02 -3.42626199e-02]\n",
      "  [ 3.46593112e-01  1.47588208e-01  1.85095295e-01 -3.00038010e-01\n",
      "   -1.77030340e-01  2.55110919e-01  5.52462675e-02 -8.52375776e-02\n",
      "   -2.05450118e-01 -2.22148141e-03 -1.89688653e-01  2.39553079e-01\n",
      "   -2.33088121e-01  3.68944049e-01  6.27546608e-02 -2.94376940e-01\n",
      "    2.63357699e-01  6.42226860e-02 -1.01515226e-01  2.43989497e-01\n",
      "    4.09613818e-01  2.17458606e-01 -3.11386496e-01  4.01901484e-01\n",
      "   -1.01735026e-01  2.60679781e-01 -3.13878246e-02 -1.20257840e-01\n",
      "   -6.28837347e-02  2.01252401e-01 -2.06352845e-01 -3.94114524e-01]]], shape=(1, 6, 32), dtype=float32)\n",
      "\n",
      " Querie splited: \n",
      "\n",
      "tf.Tensor(\n",
      "[[[[-2.82609463e-01 -2.08682790e-02 -2.31040176e-02  3.51857916e-02\n",
      "     3.03702261e-02  1.22988544e-01  3.15239400e-01 -3.86683613e-01\n",
      "    -8.36746320e-02  2.23775938e-01 -1.87846497e-01  1.48192465e-01\n",
      "     1.59224451e-01 -2.30049044e-01 -2.81861909e-02 -2.27313593e-01]\n",
      "   [-5.42927802e-01 -1.32470816e-01 -9.11547318e-02  3.31168436e-02\n",
      "    -4.67874967e-02  1.78551868e-01  5.77106714e-01 -7.02008009e-01\n",
      "    -6.22923933e-02  3.94008428e-01 -2.82211602e-01  2.00737268e-01\n",
      "     3.14434081e-01 -4.53216732e-01  1.59310475e-02 -4.11998689e-01]\n",
      "   [-3.98145616e-01 -2.74146616e-01 -1.56456620e-01 -4.17773984e-02\n",
      "    -2.35745743e-01  2.03369800e-02  3.68589848e-01 -4.36517477e-01\n",
      "     1.42963722e-01  2.20167264e-01 -5.62317669e-02 -1.36593347e-02\n",
      "     2.46954888e-01 -3.53939652e-01  1.45957261e-01 -2.54807860e-01]\n",
      "   [-1.77036971e-01  2.03516260e-01  9.51785520e-02  1.02879956e-01\n",
      "     2.65511930e-01  2.12381452e-01  2.64313847e-01 -3.38028550e-01\n",
      "    -2.83252746e-01  2.24333674e-01 -3.02106440e-01  2.89677769e-01\n",
      "     7.97119439e-02 -1.17702350e-01 -1.81962520e-01 -2.00305104e-01]\n",
      "   [-3.96515876e-02 -3.73325136e-04 -1.94830413e-03  5.89021528e-03\n",
      "     7.16834376e-03  1.88521966e-02  4.50180545e-02 -5.53836189e-02\n",
      "    -1.44626312e-02  3.23894881e-02 -2.85311751e-02  2.31139045e-02\n",
      "     2.21037511e-02 -3.19656022e-02 -5.89260878e-03 -3.25762890e-02]\n",
      "   [-4.26641434e-01 -2.97512829e-01 -1.69550508e-01 -4.61652987e-02\n",
      "    -2.56880552e-01  1.94523167e-02  3.93814623e-01 -4.66103137e-01\n",
      "     1.57187387e-01  2.34469816e-01 -5.70671931e-02 -1.80407353e-02\n",
      "     2.64976174e-01 -3.79728287e-01  1.59244746e-01 -2.72043467e-01]]\n",
      "\n",
      "  [[-6.64567500e-02  3.99258256e-01  9.34710428e-02  1.09052934e-01\n",
      "     2.32163887e-03  1.66981947e-02  8.50380734e-02 -1.26079991e-01\n",
      "    -6.93730265e-02 -6.38409331e-02  6.91394359e-02  7.40031973e-02\n",
      "     1.13783553e-01 -2.15226129e-01  1.99367285e-01 -2.64025182e-01]\n",
      "   [-6.53296337e-02  7.31058955e-01  1.88000575e-01  1.83757246e-01\n",
      "    -3.98740992e-02  1.07610293e-01  6.98466599e-02 -2.60234207e-01\n",
      "    -1.29348919e-01 -6.74001426e-02  4.82236110e-02  1.07714690e-01\n",
      "     2.64694631e-01 -3.50822002e-01  4.04061079e-01 -4.53015447e-01]\n",
      "   [ 7.15398490e-02  4.67197359e-01  1.54020682e-01  8.54233727e-02\n",
      "    -1.14185870e-01  2.23632157e-01 -1.27968594e-01 -2.25362152e-01\n",
      "    -8.73346776e-02  5.64258136e-02 -1.26734212e-01  1.29750324e-02\n",
      "     2.82441258e-01 -1.37222603e-01  3.36645901e-01 -2.28343800e-01]\n",
      "   [-1.87794134e-01  3.34432811e-01  3.87866609e-02  1.28679439e-01\n",
      "     1.05397657e-01 -1.66624770e-01  2.72536784e-01 -3.67351659e-02\n",
      "    -5.26607484e-02 -1.69519082e-01  2.41663530e-01  1.27138436e-01\n",
      "    -3.68103459e-02 -2.81720310e-01  7.55342618e-02 -2.92491585e-01]\n",
      "   [-1.10481838e-02  5.70125878e-02  1.28813302e-02  1.60126816e-02\n",
      "     1.55171950e-03  2.54175713e-04  1.45174647e-02 -1.71914045e-02\n",
      "    -9.84194502e-03 -1.04849488e-02  1.20401308e-02  1.13358144e-02\n",
      "     1.46895554e-02 -3.19299363e-02  2.73901261e-02 -3.85431796e-02]\n",
      "   [ 7.91874602e-02  4.99177217e-01  1.65385962e-01  9.04934332e-02\n",
      "    -1.24155678e-01  2.42699936e-01 -1.40918970e-01 -2.42222145e-01\n",
      "    -9.34261978e-02  6.27040341e-02 -1.39234617e-01  1.25068454e-02\n",
      "     3.04524988e-01 -1.44503742e-01  3.61593574e-01 -2.42488950e-01]]]], shape=(1, 2, 6, 16), dtype=float32)\n",
      "\n",
      " Key splited: \n",
      "\n",
      "tf.Tensor(\n",
      "[[[[-0.19857809 -0.14171208 -0.05419081  0.04152012 -0.1293508\n",
      "    -0.03076985  0.01879204  0.03717022  0.21034594 -0.00192696\n",
      "     0.00660674 -0.16511318 -0.04142038  0.31210625  0.08237282\n",
      "    -0.3880453 ]\n",
      "   [-0.36612555 -0.18367968 -0.06365234  0.0879118  -0.27404824\n",
      "     0.03877728 -0.04131875  0.02470064  0.35424134  0.06689715\n",
      "    -0.06122686 -0.25439733 -0.04940116  0.58509856  0.19324271\n",
      "    -0.70917195]\n",
      "   [-0.23904665  0.03499711  0.03083392  0.0800772  -0.24992082\n",
      "     0.21599494 -0.17863908 -0.0713793   0.16424507  0.18432641\n",
      "    -0.1865296  -0.06622115  0.02158353  0.40129575  0.20876029\n",
      "    -0.45048487]\n",
      "   [-0.16042645 -0.2964218  -0.12879519  0.00690997 -0.02112875\n",
      "    -0.24878202  0.19328748  0.13279335  0.24866536 -0.16672927\n",
      "     0.177445   -0.25068292 -0.09668784  0.22950178 -0.03044404\n",
      "    -0.3282188 ]\n",
      "   [-0.02828651 -0.0223321  -0.00872196  0.00560021 -0.01744208\n",
      "    -0.00702414  0.00477755  0.0065068   0.0308914  -0.00222265\n",
      "     0.00297106 -0.02490302 -0.00664586  0.04419101  0.01058964\n",
      "    -0.05544891]\n",
      "   [-0.2555325   0.04109249  0.03468083  0.08613871 -0.2688438\n",
      "     0.23542257 -0.19456327 -0.07838161  0.17397897  0.20038107\n",
      "    -0.20287655 -0.06841448  0.02435152  0.42942935  0.22512026\n",
      "    -0.4812546 ]]\n",
      "\n",
      "  [[-0.11377507 -0.33995554  0.11080513 -0.16393432 -0.0275334\n",
      "    -0.25983188  0.12101447 -0.06567758 -0.2642448   0.3756424\n",
      "    -0.16016002  0.26870462 -0.19108936  0.30910844 -0.00081298\n",
      "    -0.05797506]\n",
      "   [-0.2776431  -0.6023139   0.24635272 -0.33738387 -0.14484933\n",
      "    -0.509746    0.17709115 -0.06379591 -0.49881873  0.6820208\n",
      "    -0.34850967  0.5041742  -0.33220163  0.55365574 -0.06133844\n",
      "    -0.17298432]\n",
      "   [-0.3167775  -0.34439468  0.24481054 -0.29042023 -0.28240794\n",
      "    -0.39407757  0.02373343  0.07273519 -0.348884    0.42420593\n",
      "    -0.33378905  0.34665617 -0.1767358   0.32902762 -0.15951405\n",
      "    -0.24489449]\n",
      "   [ 0.06721213 -0.3320229  -0.00908813 -0.05006963  0.1983422\n",
      "    -0.13797067  0.20567799 -0.18739244 -0.18623088  0.32824156\n",
      "    -0.0046205   0.19655642 -0.2015408   0.28784007  0.13963917\n",
      "     0.10812221]\n",
      "   [-0.01432985 -0.04910185  0.01462065 -0.02238015 -0.00132026\n",
      "    -0.03616329  0.01851074 -0.01093988 -0.03731906  0.05380062\n",
      "    -0.02134242  0.03803363 -0.02777606  0.04448063  0.00153895\n",
      "    -0.00643058]\n",
      "   [-0.34184435 -0.36698472  0.26368934 -0.312116   -0.30634817\n",
      "    -0.42271104  0.02318638  0.08046985 -0.3734962   0.45296007\n",
      "    -0.35933375  0.37097868 -0.18796994  0.35094765 -0.1733541\n",
      "    -0.26491952]]]], shape=(1, 2, 6, 16), dtype=float32)\n",
      "\n",
      " Value splited: \n",
      "\n",
      "tf.Tensor(\n",
      "[[[[ 1.92282185e-01  9.74319428e-02  1.43936142e-01 -1.21906511e-01\n",
      "    -1.68756455e-01  1.14047974e-01 -8.63031372e-02  1.13671504e-01\n",
      "    -7.17714429e-02 -1.30521446e-01 -1.88675150e-03  1.63959354e-01\n",
      "    -1.16673067e-01  2.52246350e-01  7.46425688e-02 -1.67255431e-01]\n",
      "   [ 3.88954878e-01  1.87352508e-01  2.65337378e-01 -2.74482906e-01\n",
      "    -2.97206819e-01  2.47903332e-01 -1.01366378e-01  1.29180416e-01\n",
      "    -1.71603069e-01 -1.83090225e-01 -6.85110241e-02  3.12219024e-01\n",
      "    -2.43922070e-01  4.80477840e-01  1.26057982e-01 -3.35862547e-01]\n",
      "   [ 3.22703898e-01  1.37724057e-01  1.73155636e-01 -2.78474033e-01\n",
      "    -1.66227624e-01  2.36982062e-01  4.91186157e-02 -7.61698559e-02\n",
      "    -1.90452114e-01 -4.63281851e-03 -1.74564287e-01  2.23657772e-01\n",
      "    -2.16771632e-01  3.44457716e-01  5.92192262e-02 -2.74164915e-01]\n",
      "   [ 7.46006370e-02  6.06278144e-02  1.16382971e-01  1.80838667e-02\n",
      "    -1.69006631e-01  3.91409779e-03 -2.05128133e-01  2.80332804e-01\n",
      "     3.41004245e-02 -2.40389273e-01  1.50946528e-01  1.09197997e-01\n",
      "    -2.67165620e-02  1.67672768e-01  8.74122754e-02 -7.06758201e-02]\n",
      "   [ 2.64373887e-02  1.36654107e-02  2.05042046e-02 -1.59901045e-02\n",
      "    -2.44239196e-02  1.52050350e-02 -1.38905458e-02  1.84152760e-02\n",
      "    -9.13741067e-03 -2.01837886e-02  1.52959139e-03  2.30808780e-02\n",
      "    -1.58229005e-02  3.55053805e-02  1.09522389e-02 -2.30646245e-02]\n",
      "   [ 3.46593112e-01  1.47588208e-01  1.85095295e-01 -3.00038010e-01\n",
      "    -1.77030340e-01  2.55110919e-01  5.52462675e-02 -8.52375776e-02\n",
      "    -2.05450118e-01 -2.22148141e-03 -1.89688653e-01  2.39553079e-01\n",
      "    -2.33088121e-01  3.68944049e-01  6.27546608e-02 -2.94376940e-01]]\n",
      "\n",
      "  [[-3.23836654e-02 -1.32410690e-01 -6.25965465e-03  4.69313674e-02\n",
      "     2.27472156e-01  1.43550456e-01 -1.21982127e-01  3.17087442e-01\n",
      "    -1.82999209e-01  5.65046631e-02  4.60591651e-02 -1.20136619e-01\n",
      "    -1.46330461e-01  2.44315401e-01 -1.51730791e-01 -2.45779827e-01]\n",
      "   [ 4.62244079e-02 -1.62654325e-01 -4.39982414e-02  1.50289282e-01\n",
      "     4.59995925e-01  2.76037842e-01 -2.78529674e-01  5.82496583e-01\n",
      "    -2.90953040e-01  1.69457972e-01  5.34372590e-02 -2.09576145e-01\n",
      "    -2.26239964e-01  4.11163151e-01 -2.83607721e-01 -4.80186343e-01]\n",
      "   [ 2.41665184e-01  5.64629920e-02 -9.35252756e-02  2.25418329e-01\n",
      "     3.81385356e-01  2.02924490e-01 -2.88916916e-01  3.76066983e-01\n",
      "    -9.72331539e-02  2.40964442e-01 -2.79654134e-02 -1.13028564e-01\n",
      "    -6.07599169e-02  1.90012336e-01 -1.92868680e-01 -3.67488027e-01]\n",
      "   [-2.74521887e-01 -2.97994912e-01  7.10398182e-02 -1.11573517e-01\n",
      "     8.85873064e-02  8.93165469e-02  2.71836445e-02  2.61158615e-01\n",
      "    -2.56742448e-01 -1.07398622e-01  1.11024730e-01 -1.25011861e-01\n",
      "    -2.20332578e-01  2.89493233e-01 -1.13538623e-01 -1.35179177e-01]\n",
      "   [-7.54223438e-03 -2.11143419e-02  5.88250350e-06  4.92198020e-03\n",
      "     3.12796868e-02  2.01337002e-02 -1.58928279e-02  4.52264771e-02\n",
      "    -2.73518357e-02  6.24367129e-03  7.43153226e-03 -1.74426306e-02\n",
      "    -2.20485069e-02  3.58880609e-02 -2.15066951e-02 -3.42626199e-02]\n",
      "   [ 2.63357699e-01  6.42226860e-02 -1.01515226e-01  2.43989497e-01\n",
      "     4.09613818e-01  2.17458606e-01 -3.11386496e-01  4.01901484e-01\n",
      "    -1.01735026e-01  2.60679781e-01 -3.13878246e-02 -1.20257840e-01\n",
      "    -6.28837347e-02  2.01252401e-01 -2.06352845e-01 -3.94114524e-01]]]], shape=(1, 2, 6, 16), dtype=float32)\n",
      "Applying SELF ATTENTION:\n",
      "-------------------------------------------------\n",
      "\n",
      "multiplying QUERIE and KEY:\n",
      "tf.Tensor(\n",
      "[[[[ 9.37829912e-03  5.81579655e-02  1.19559824e-01 -8.82373154e-02\n",
      "     2.05799937e-04  1.29744217e-01]\n",
      "   [ 7.60499015e-02  1.95435941e-01  2.37845033e-01 -6.80264980e-02\n",
      "     9.30594001e-03  2.56867945e-01]\n",
      "   [ 1.66961610e-01  3.03703278e-01  1.90045655e-01  1.44566804e-01\n",
      "     2.38970909e-02  2.02956200e-01]\n",
      "   [-1.30185515e-01 -1.59822449e-01  5.57751060e-02 -2.93218076e-01\n",
      "    -2.07622424e-02  6.34270459e-02]\n",
      "   [-2.88967974e-04  5.84509782e-03  1.65493339e-02 -1.51866237e-02\n",
      "    -2.17530411e-04  1.79932639e-02]\n",
      "   [ 1.81263939e-01  3.28833252e-01  2.03978091e-01  1.59028053e-01\n",
      "     2.59686820e-02  2.17790678e-01]]\n",
      "\n",
      "  [[-1.91485077e-01 -3.40777129e-01 -1.97998583e-01 -1.83465540e-01\n",
      "    -2.76154689e-02 -2.11071402e-01]\n",
      "   [-3.54711980e-01 -6.30510569e-01 -3.64780545e-01 -3.41623694e-01\n",
      "    -5.11764623e-02 -3.88823509e-01]\n",
      "   [-2.34916732e-01 -4.16073263e-01 -2.37616524e-01 -2.29760513e-01\n",
      "    -3.39342877e-02 -2.53194779e-01]\n",
      "   [-1.50794804e-01 -2.70129859e-01 -1.60606503e-01 -1.40335768e-01\n",
      "    -2.16983538e-02 -1.71308100e-01]\n",
      "   [-2.72301286e-02 -4.84810434e-02 -2.82116029e-02 -2.60408316e-02\n",
      "    -3.92647972e-03 -3.00754197e-02]\n",
      "   [-2.51196742e-01 -4.44872499e-01 -2.53990650e-01 -2.45765492e-01\n",
      "    -3.62869464e-02 -2.70640433e-01]]]], shape=(1, 2, 6, 6), dtype=float32)\n",
      "\n",
      "Taking the dk size:\n",
      "tf.Tensor(16.0, shape=(), dtype=float32)\n",
      "\n",
      "Standarding the process:\n",
      "tf.Tensor(\n",
      "[[[[ 2.34457478e-03  1.45394914e-02  2.98899561e-02 -2.20593289e-02\n",
      "     5.14499843e-05  3.24360542e-02]\n",
      "   [ 1.90124754e-02  4.88589853e-02  5.94612584e-02 -1.70066245e-02\n",
      "     2.32648500e-03  6.42169863e-02]\n",
      "   [ 4.17404026e-02  7.59258196e-02  4.75114137e-02  3.61417010e-02\n",
      "     5.97427273e-03  5.07390499e-02]\n",
      "   [-3.25463787e-02 -3.99556123e-02  1.39437765e-02 -7.33045191e-02\n",
      "    -5.19056059e-03  1.58567615e-02]\n",
      "   [-7.22419936e-05  1.46127446e-03  4.13733348e-03 -3.79665592e-03\n",
      "    -5.43826027e-05  4.49831598e-03]\n",
      "   [ 4.53159846e-02  8.22083130e-02  5.09945229e-02  3.97570133e-02\n",
      "     6.49217051e-03  5.44476695e-02]]\n",
      "\n",
      "  [[-4.78712693e-02 -8.51942822e-02 -4.94996458e-02 -4.58663851e-02\n",
      "    -6.90386724e-03 -5.27678505e-02]\n",
      "   [-8.86779949e-02 -1.57627642e-01 -9.11951363e-02 -8.54059234e-02\n",
      "    -1.27941156e-02 -9.72058773e-02]\n",
      "   [-5.87291829e-02 -1.04018316e-01 -5.94041310e-02 -5.74401282e-02\n",
      "    -8.48357193e-03 -6.32986948e-02]\n",
      "   [-3.76987010e-02 -6.75324649e-02 -4.01516259e-02 -3.50839421e-02\n",
      "    -5.42458845e-03 -4.28270251e-02]\n",
      "   [-6.80753216e-03 -1.21202609e-02 -7.05290074e-03 -6.51020790e-03\n",
      "    -9.81619931e-04 -7.51885492e-03]\n",
      "   [-6.27991855e-02 -1.11218125e-01 -6.34976625e-02 -6.14413731e-02\n",
      "    -9.07173660e-03 -6.76601082e-02]]]], shape=(1, 2, 6, 6), dtype=float32)\n",
      "\n",
      "Applying SOFTMAX in the Standarded vlaues, CALCULATING attention weights:\n",
      "tf.Tensor(\n",
      "[[[[0.16544384 0.16747376 0.17006439 0.16145523 0.1650649  0.17049794]\n",
      "   [0.16485627 0.16985081 0.1716612  0.15902396 0.1621283  0.17247951]\n",
      "   [0.1664202  0.17220771 0.16738339 0.16549106 0.16057318 0.16792451]\n",
      "   [0.16453879 0.16332418 0.17236881 0.15796733 0.16910201 0.17269887]\n",
      "   [0.16648258 0.1667381  0.16718489 0.16586368 0.16648556 0.16724524]\n",
      "   [0.16642186 0.1726762  0.16736957 0.1654993  0.16008455 0.16794853]]\n",
      "\n",
      "  [[0.16664782 0.16054267 0.16637668 0.16698226 0.17361672 0.16583383]\n",
      "   [0.16654225 0.15544614 0.16612357 0.16708806 0.17967197 0.16512802]\n",
      "   [0.16657487 0.15919912 0.16646248 0.16678974 0.17515837 0.16581544]\n",
      "   [0.16670944 0.16180934 0.16630101 0.16714591 0.17217761 0.16585669]\n",
      "   [0.16666985 0.16578673 0.16662896 0.16671942 0.1676437  0.16655134]\n",
      "   [0.16656275 0.1586901  0.16644645 0.16678907 0.17575651 0.16575508]]]], shape=(1, 2, 6, 6), dtype=float32)\n",
      "\n",
      "Calculating the outpu, multipling the attention weights and the VALUE MATRIX:\n",
      "tf.Tensor(\n",
      "[[[[ 0.22733404  0.10812594  0.15143153 -0.16437142 -0.1674653\n",
      "     0.1473257  -0.04889368  0.06125488 -0.10403367 -0.09556716\n",
      "    -0.04919106  0.17973453 -0.14368491  0.27661657  0.07015218\n",
      "    -0.19595402]\n",
      "   [ 0.22908872  0.10883889  0.15227778 -0.16598848 -0.16820624\n",
      "     0.14867774 -0.0483565   0.06046899 -0.10516672 -0.09529376\n",
      "    -0.05037891  0.1808789  -0.14489277  0.27837968  0.07038219\n",
      "    -0.19743568]\n",
      "   [ 0.22778828  0.10854225  0.1522652  -0.1641263  -0.16870818\n",
      "     0.14726627 -0.05049714  0.06344962 -0.10369813 -0.0974227\n",
      "    -0.04795874  0.18049356 -0.14380929  0.27778167  0.07080514\n",
      "    -0.19639634]\n",
      "   [ 0.22689901  0.10774624  0.15068348 -0.16455181 -0.1663611\n",
      "     0.1473491  -0.04750077  0.0593494  -0.10430352 -0.09394787\n",
      "    -0.05024513  0.1790455  -0.14355037  0.2755588   0.06957544\n",
      "    -0.19553527]\n",
      "   [ 0.22555748  0.10749938  0.15082738 -0.16246131 -0.16714722\n",
      "     0.14578846 -0.05015392  0.0630365  -0.10262796 -0.09663589\n",
      "    -0.04735534  0.1787661  -0.14238456  0.27512282  0.07016324\n",
      "    -0.19447805]\n",
      "   [ 0.22796239  0.10862565  0.15238275 -0.1642505  -0.16883911\n",
      "     0.14737804 -0.05053902  0.06350265 -0.1037762  -0.0975008\n",
      "    -0.04799249  0.1806324  -0.14391884  0.27799535  0.07086038\n",
      "    -0.19654655]]\n",
      "\n",
      "  [[ 0.03875564 -0.08156016 -0.02863843  0.09213866  0.26336136\n",
      "     0.15647194 -0.16297148  0.32703608 -0.1578754   0.10309234\n",
      "     0.02622606 -0.11631766 -0.12186372  0.22628295 -0.15981874\n",
      "    -0.2730691 ]\n",
      "   [ 0.03820172 -0.08093621 -0.02831065  0.09115648  0.26080608\n",
      "     0.15497647 -0.16133952  0.3239565  -0.1564696   0.1020042\n",
      "     0.02603484 -0.11524221 -0.1207923   0.22421944 -0.15830508\n",
      "    -0.27044648]\n",
      "   [ 0.03875302 -0.08130348 -0.02859868  0.09197724  0.26278308\n",
      "     0.15611786 -0.16263716  0.32627463 -0.15747035  0.10290673\n",
      "     0.02613916 -0.11603762 -0.12154471  0.22572489 -0.15945068\n",
      "    -0.27245712]\n",
      "   [ 0.03876587 -0.08179553 -0.02867817  0.0922951   0.26390803\n",
      "     0.15680571 -0.16328973  0.32775182 -0.15825285  0.10327163\n",
      "     0.02630546 -0.11658007 -0.12216048  0.22680473 -0.16016507\n",
      "    -0.27364653]\n",
      "   [ 0.03936447 -0.08215128 -0.02898443  0.09315968  0.2659586\n",
      "     0.15798616 -0.16464332  0.33014214 -0.15927188  0.104221\n",
      "     0.02640416 -0.11739708 -0.1229242   0.22834635 -0.16134775\n",
      "    -0.27572796]\n",
      "   [ 0.03870578 -0.08123629 -0.02856863  0.09188484  0.26253402\n",
      "     0.1559712  -0.16248     0.3259709  -0.15732853  0.102804\n",
      "     0.02611812 -0.11593077 -0.12143605  0.2255187  -0.15930171\n",
      "    -0.27220044]]]], shape=(1, 2, 6, 16), dtype=float32)\n",
      "-------------------------------------------------\n",
      "\n",
      " Transposing the output self attention \n",
      "\n",
      "tf.Tensor(\n",
      "[[[[ 0.22733404  0.10812594  0.15143153 -0.16437142 -0.1674653\n",
      "     0.1473257  -0.04889368  0.06125488 -0.10403367 -0.09556716\n",
      "    -0.04919106  0.17973453 -0.14368491  0.27661657  0.07015218\n",
      "    -0.19595402]\n",
      "   [ 0.03875564 -0.08156016 -0.02863843  0.09213866  0.26336136\n",
      "     0.15647194 -0.16297148  0.32703608 -0.1578754   0.10309234\n",
      "     0.02622606 -0.11631766 -0.12186372  0.22628295 -0.15981874\n",
      "    -0.2730691 ]]\n",
      "\n",
      "  [[ 0.22908872  0.10883889  0.15227778 -0.16598848 -0.16820624\n",
      "     0.14867774 -0.0483565   0.06046899 -0.10516672 -0.09529376\n",
      "    -0.05037891  0.1808789  -0.14489277  0.27837968  0.07038219\n",
      "    -0.19743568]\n",
      "   [ 0.03820172 -0.08093621 -0.02831065  0.09115648  0.26080608\n",
      "     0.15497647 -0.16133952  0.3239565  -0.1564696   0.1020042\n",
      "     0.02603484 -0.11524221 -0.1207923   0.22421944 -0.15830508\n",
      "    -0.27044648]]\n",
      "\n",
      "  [[ 0.22778828  0.10854225  0.1522652  -0.1641263  -0.16870818\n",
      "     0.14726627 -0.05049714  0.06344962 -0.10369813 -0.0974227\n",
      "    -0.04795874  0.18049356 -0.14380929  0.27778167  0.07080514\n",
      "    -0.19639634]\n",
      "   [ 0.03875302 -0.08130348 -0.02859868  0.09197724  0.26278308\n",
      "     0.15611786 -0.16263716  0.32627463 -0.15747035  0.10290673\n",
      "     0.02613916 -0.11603762 -0.12154471  0.22572489 -0.15945068\n",
      "    -0.27245712]]\n",
      "\n",
      "  [[ 0.22689901  0.10774624  0.15068348 -0.16455181 -0.1663611\n",
      "     0.1473491  -0.04750077  0.0593494  -0.10430352 -0.09394787\n",
      "    -0.05024513  0.1790455  -0.14355037  0.2755588   0.06957544\n",
      "    -0.19553527]\n",
      "   [ 0.03876587 -0.08179553 -0.02867817  0.0922951   0.26390803\n",
      "     0.15680571 -0.16328973  0.32775182 -0.15825285  0.10327163\n",
      "     0.02630546 -0.11658007 -0.12216048  0.22680473 -0.16016507\n",
      "    -0.27364653]]\n",
      "\n",
      "  [[ 0.22555748  0.10749938  0.15082738 -0.16246131 -0.16714722\n",
      "     0.14578846 -0.05015392  0.0630365  -0.10262796 -0.09663589\n",
      "    -0.04735534  0.1787661  -0.14238456  0.27512282  0.07016324\n",
      "    -0.19447805]\n",
      "   [ 0.03936447 -0.08215128 -0.02898443  0.09315968  0.2659586\n",
      "     0.15798616 -0.16464332  0.33014214 -0.15927188  0.104221\n",
      "     0.02640416 -0.11739708 -0.1229242   0.22834635 -0.16134775\n",
      "    -0.27572796]]\n",
      "\n",
      "  [[ 0.22796239  0.10862565  0.15238275 -0.1642505  -0.16883911\n",
      "     0.14737804 -0.05053902  0.06350265 -0.1037762  -0.0975008\n",
      "    -0.04799249  0.1806324  -0.14391884  0.27799535  0.07086038\n",
      "    -0.19654655]\n",
      "   [ 0.03870578 -0.08123629 -0.02856863  0.09188484  0.26253402\n",
      "     0.1559712  -0.16248     0.3259709  -0.15732853  0.102804\n",
      "     0.02611812 -0.11593077 -0.12143605  0.2255187  -0.15930171\n",
      "    -0.27220044]]]], shape=(1, 6, 2, 16), dtype=float32)\n",
      "\n",
      " Concating the Transposed output self attention \n",
      "\n",
      "tf.Tensor(\n",
      "[[[ 0.22733404  0.10812594  0.15143153 -0.16437142 -0.1674653\n",
      "    0.1473257  -0.04889368  0.06125488 -0.10403367 -0.09556716\n",
      "   -0.04919106  0.17973453 -0.14368491  0.27661657  0.07015218\n",
      "   -0.19595402  0.03875564 -0.08156016 -0.02863843  0.09213866\n",
      "    0.26336136  0.15647194 -0.16297148  0.32703608 -0.1578754\n",
      "    0.10309234  0.02622606 -0.11631766 -0.12186372  0.22628295\n",
      "   -0.15981874 -0.2730691 ]\n",
      "  [ 0.22908872  0.10883889  0.15227778 -0.16598848 -0.16820624\n",
      "    0.14867774 -0.0483565   0.06046899 -0.10516672 -0.09529376\n",
      "   -0.05037891  0.1808789  -0.14489277  0.27837968  0.07038219\n",
      "   -0.19743568  0.03820172 -0.08093621 -0.02831065  0.09115648\n",
      "    0.26080608  0.15497647 -0.16133952  0.3239565  -0.1564696\n",
      "    0.1020042   0.02603484 -0.11524221 -0.1207923   0.22421944\n",
      "   -0.15830508 -0.27044648]\n",
      "  [ 0.22778828  0.10854225  0.1522652  -0.1641263  -0.16870818\n",
      "    0.14726627 -0.05049714  0.06344962 -0.10369813 -0.0974227\n",
      "   -0.04795874  0.18049356 -0.14380929  0.27778167  0.07080514\n",
      "   -0.19639634  0.03875302 -0.08130348 -0.02859868  0.09197724\n",
      "    0.26278308  0.15611786 -0.16263716  0.32627463 -0.15747035\n",
      "    0.10290673  0.02613916 -0.11603762 -0.12154471  0.22572489\n",
      "   -0.15945068 -0.27245712]\n",
      "  [ 0.22689901  0.10774624  0.15068348 -0.16455181 -0.1663611\n",
      "    0.1473491  -0.04750077  0.0593494  -0.10430352 -0.09394787\n",
      "   -0.05024513  0.1790455  -0.14355037  0.2755588   0.06957544\n",
      "   -0.19553527  0.03876587 -0.08179553 -0.02867817  0.0922951\n",
      "    0.26390803  0.15680571 -0.16328973  0.32775182 -0.15825285\n",
      "    0.10327163  0.02630546 -0.11658007 -0.12216048  0.22680473\n",
      "   -0.16016507 -0.27364653]\n",
      "  [ 0.22555748  0.10749938  0.15082738 -0.16246131 -0.16714722\n",
      "    0.14578846 -0.05015392  0.0630365  -0.10262796 -0.09663589\n",
      "   -0.04735534  0.1787661  -0.14238456  0.27512282  0.07016324\n",
      "   -0.19447805  0.03936447 -0.08215128 -0.02898443  0.09315968\n",
      "    0.2659586   0.15798616 -0.16464332  0.33014214 -0.15927188\n",
      "    0.104221    0.02640416 -0.11739708 -0.1229242   0.22834635\n",
      "   -0.16134775 -0.27572796]\n",
      "  [ 0.22796239  0.10862565  0.15238275 -0.1642505  -0.16883911\n",
      "    0.14737804 -0.05053902  0.06350265 -0.1037762  -0.0975008\n",
      "   -0.04799249  0.1806324  -0.14391884  0.27799535  0.07086038\n",
      "   -0.19654655  0.03870578 -0.08123629 -0.02856863  0.09188484\n",
      "    0.26253402  0.1559712  -0.16248     0.3259709  -0.15732853\n",
      "    0.102804    0.02611812 -0.11593077 -0.12143605  0.2255187\n",
      "   -0.15930171 -0.27220044]]], shape=(1, 6, 32), dtype=float32)\n",
      "\n",
      " Applying into a dense layer the Concat Transposed output self attention \n",
      "\n",
      "tf.Tensor(\n",
      "[[[ 0.10180404 -0.02025091 -0.11517724 -0.09478576  0.03463305\n",
      "    0.15767415  0.07031569  0.20829412  0.16316785 -0.11583101\n",
      "    0.02498591  0.11777857  0.14957936 -0.07791815 -0.08480829\n",
      "   -0.06444649 -0.03962015 -0.01418196  0.06543678 -0.21337223\n",
      "   -0.25678626  0.0419296  -0.28222498 -0.00819264  0.17986147\n",
      "   -0.21845064  0.03244831  0.161188   -0.07442721 -0.08793357\n",
      "    0.20781654  0.07603166]\n",
      "  [ 0.10182843 -0.02007929 -0.11461154 -0.09364606  0.03334269\n",
      "    0.15663168  0.07005162  0.21053284  0.16452172 -0.11831354\n",
      "    0.02467693  0.1179994   0.14748028 -0.07926399 -0.08309267\n",
      "   -0.06591127 -0.03871091 -0.01663613  0.0660552  -0.21332264\n",
      "   -0.25699595  0.04237688 -0.28152972 -0.00872803  0.17958082\n",
      "   -0.21998532  0.03347231  0.16049847 -0.0741125  -0.0879523\n",
      "    0.20757498  0.07697756]\n",
      "  [ 0.10340371 -0.02051263 -0.11503969 -0.09438246  0.03473271\n",
      "    0.15864219  0.07056937  0.21056946  0.16345581 -0.11721891\n",
      "    0.02516044  0.11839364  0.14974041 -0.07916473 -0.08488468\n",
      "   -0.06455296 -0.0393533  -0.01404735  0.06568538 -0.21270584\n",
      "   -0.2575807   0.04190837 -0.2819867  -0.00920035  0.18011372\n",
      "   -0.2185787   0.03272267  0.16036981 -0.07502324 -0.08841022\n",
      "    0.2086469   0.07586452]\n",
      "  [ 0.10052549 -0.01992167 -0.1153504  -0.09511098  0.0343493\n",
      "    0.15678541  0.07025639  0.20613512  0.16296959 -0.11448525\n",
      "    0.02487154  0.1172228   0.14940687 -0.07699885 -0.08474357\n",
      "   -0.06433214 -0.03993858 -0.01421602  0.06540348 -0.21389467\n",
      "   -0.25599986  0.04188465 -0.28242543 -0.00728723  0.1795115\n",
      "   -0.21828808  0.03221506  0.16174375 -0.07383178 -0.08753751\n",
      "    0.20727853  0.07623553]\n",
      "  [ 0.10244447 -0.02052725 -0.11580935 -0.0959261   0.03596083\n",
      "    0.15905663  0.07078071  0.20656942  0.161832   -0.11355598\n",
      "    0.02548371  0.11771983  0.15195617 -0.07706791 -0.0866622\n",
      "   -0.062851   -0.04056079 -0.01137285  0.06498214 -0.21313965\n",
      "   -0.25675994  0.04139888 -0.28286058 -0.00795005  0.18032233\n",
      "   -0.21675564  0.03138321  0.1614706  -0.07499328 -0.08808281\n",
      "    0.20864096  0.07500181]\n",
      "  [ 0.10346836 -0.02053405 -0.11499134 -0.09429332  0.03467637\n",
      "    0.15863244  0.07058644  0.21087076  0.16358842 -0.11751673\n",
      "    0.02512309  0.11844905  0.14954162 -0.07933859 -0.08473878\n",
      "   -0.06467628 -0.03925934 -0.01428498  0.06574778 -0.21266147\n",
      "   -0.25769606  0.04194357 -0.28190282 -0.00929024  0.1800793\n",
      "   -0.21869569  0.03283899  0.16033603 -0.07502513 -0.08842195\n",
      "    0.20865586  0.07590833]]], shape=(1, 6, 32), dtype=float32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(TensorShape([1, 6, 32]), TensorShape([1, 2, 6, 6]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out, attn = temp_mha(y, k=y, q=y, mask=None)\n",
    "out.shape, attn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InterpretableMultiHeadSelfAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, heads, units, dropout_rate):\n",
    "        super().__init__()\n",
    "        d_att = units // heads\n",
    "        self._queries = tf.keras.layers.Dense(heads*d_att, use_bias=False)\n",
    "        self._keys = tf.keras.layers.Dense(heads*d_att, use_bias=False)\n",
    "        self._values = tf.keras.layers.Dense(d_att, use_bias=False)\n",
    "\n",
    "        self._reshape = tf.keras.layers.TimeDistributed(tf.keras.layers.Reshape([heads, d_att]))\n",
    "        self._softmax = tf.keras.layers.Softmax()\n",
    "        \n",
    "        self._attention = tf.keras.layers.Attention(dropout=dropout_rate)\n",
    "        self._out_weights = tf.keras.layers.Dense(units, use_bias=False)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        query, value, key = inputs, inputs, inputs\n",
    "\n",
    "        query = self._reshape(self._queries(query))\n",
    "        key = self._reshape(self._keys(key))\n",
    "\n",
    "        value = tf.repeat(tf.expand_dims(self._values(value), -2), tf.shape(key)[-2], -2)\n",
    "\n",
    "        outputs, attention = self._attention(\n",
    "            [query/tf.sqrt(tf.cast(tf.shape(key)[-1], float)), value, key],\n",
    "            use_causal_mask=True, return_attention_scores=True\n",
    "        )\n",
    "\n",
    "        outputs = self._out_weights(tf.reduce_mean(outputs, -2))\n",
    "        attention = tf.reduce_mean(attention, -2)\n",
    "\n",
    "        return outputs, attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledDotProductAttention(tf.keras.layers.Layer):\n",
    "  def __init__(self, dropout_rate=0.0, use_scale=True, **kwargs):\n",
    "    super(ScaledDotProductAttention, self).__init__(**kwargs)\n",
    "    self.dropout = tf.keras.layers.Dropout(rate=dropout_rate)\n",
    "    self.softmax = tf.keras.layers.Softmax(axis=-1)\n",
    "    self.use_scale = use_scale\n",
    "\n",
    "  def call(self, queries, keys, values, mask=None):\n",
    "    # Attention calculation\n",
    "    attn = tf.matmul(queries, keys, transpose_b=True)\n",
    "\n",
    "    # Scaling (if enabled)\n",
    "    if self.use_scale:\n",
    "      dk = tf.cast(tf.shape(keys)[-1], tf.float32)  # Dimension of key vectors\n",
    "      attn = attn / tf.math.sqrt(dk)\n",
    "\n",
    "    # Masking (if provided)\n",
    "    if mask is not None:\n",
    "      attn = tf.where(mask, attn, tf.constant(-1e9, dtype=attn.dtype))  # Apply mask\n",
    "\n",
    "    # Apply softmax and dropout\n",
    "    attn = self.softmax(attn)\n",
    "    attn = self.dropout(attn)\n",
    "\n",
    "    # Context layer\n",
    "    output = tf.matmul(attn, values)\n",
    "\n",
    "    return output, attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(1, 6, 2), dtype=float32, numpy=\n",
       " array([[[0.43756613, 0.39426136],\n",
       "         [0.5368911 , 0.42028242],\n",
       "         [0.5006426 , 0.4020337 ],\n",
       "         [0.45021075, 0.37592688],\n",
       "         [0.44898018, 0.4064455 ],\n",
       "         [0.49814713, 0.38136753]]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(1, 6, 6), dtype=float32, numpy=\n",
       " array([[[0.16856927, 0.18274643, 0.16895472, 0.14968768, 0.18063694,\n",
       "          0.14940494],\n",
       "         [0.12075994, 0.27230155, 0.190591  , 0.11227559, 0.14429735,\n",
       "          0.15977456],\n",
       "         [0.13593362, 0.2320518 , 0.1840606 , 0.13057399, 0.15228319,\n",
       "          0.1650968 ],\n",
       "         [0.1579818 , 0.1793215 , 0.17128567, 0.16019563, 0.16054478,\n",
       "          0.17067055],\n",
       "         [0.16428305, 0.198596  , 0.17213993, 0.13834433, 0.1839302 ,\n",
       "          0.14270644],\n",
       "         [0.13309738, 0.21539623, 0.18280438, 0.14405958, 0.13978541,\n",
       "          0.18485697]]], dtype=float32)>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "teste = ScaledDotProductAttention()\n",
    "teste(y,y,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InterpretableMultiHeadAttention(tf.keras.layers.Layer):\n",
    "  def __init__(self, num_heads, d_model, dropout_rate, use_bias=False, **kwargs):\n",
    "    super(InterpretableMultiHeadAttention, self).__init__(**kwargs)\n",
    "    self.num_heads = num_heads\n",
    "    self.d_model = d_model\n",
    "    self.d_k = self.d_q = self.d_v = d_model // num_heads\n",
    "    self.dropout = tf.keras.layers.Dropout(rate=dropout_rate)\n",
    "\n",
    "    self.v_layer = tf.keras.layers.Dense(self.d_v, use_bias=use_bias)\n",
    "    self.q_layers = [tf.keras.layers.Dense(self.d_q, use_bias=use_bias) for _ in range(num_heads)]\n",
    "    self.k_layers = [tf.keras.layers.Dense(self.d_k, use_bias=use_bias) for _ in range(num_heads)]\n",
    "    self.w_h = tf.keras.layers.Dense(self.d_model, use_bias=use_bias)\n",
    "\n",
    "\n",
    "  def call(self, queries, keys, values, mask=None):\n",
    "    heads, attns = [], []\n",
    "    v = self.v_layer(values)\n",
    "\n",
    "    for i in range(self.num_heads):\n",
    "      q = self.q_layers[i](queries)\n",
    "      k = self.k_layers[i](keys)\n",
    "      \n",
    "      head, attn = scaled_dot_product_attention(q, k, v, mask)\n",
    "      head_dropout = self.dropout(head)\n",
    "      heads.append(head_dropout)\n",
    "      attns.append(attn)\n",
    "\n",
    "    \n",
    "    head = tf.concat(heads, axis=-1) if self.num_heads > 1 else heads[0]\n",
    "    attn = tf.concat(attns, axis=-1)\n",
    "\n",
    "    outputs = tf.reduce_mean(head, axis=-1) if self.num_heads > 1 else head\n",
    "    outputs = self.w_h(outputs)\n",
    "    outputs = self.dropout(outputs)\n",
    "\n",
    "    return outputs, attn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "teste = InterpretableMultiHeadAttention(num_heads=2, d_model=32, dropout_rate=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying SELF ATTENTION:\n",
      "-------------------------------------------------\n",
      "\n",
      "multiplying QUERIE and KEY:\n",
      "Tensor(\"MatMul:0\", shape=(1, 6, 6), dtype=float32)\n",
      "\n",
      "Taking the dk size:\n",
      "Tensor(\"Cast:0\", shape=(), dtype=float32)\n",
      "\n",
      "Standarding the process:\n",
      "Tensor(\"truediv:0\", shape=(1, 6, 6), dtype=float32)\n",
      "\n",
      "Applying SOFTMAX in the Standarded vlaues, CALCULATING attention weights:\n",
      "Tensor(\"Softmax:0\", shape=(1, 6, 6), dtype=float32)\n",
      "\n",
      "Calculating the outpu, multipling the attention weights and the VALUE MATRIX:\n",
      "Tensor(\"MatMul_1:0\", shape=(1, 6, 16), dtype=float32)\n",
      "-------------------------------------------------\n",
      "Applying SELF ATTENTION:\n",
      "-------------------------------------------------\n",
      "\n",
      "multiplying QUERIE and KEY:\n",
      "Tensor(\"MatMul_2:0\", shape=(1, 6, 6), dtype=float32)\n",
      "\n",
      "Taking the dk size:\n",
      "Tensor(\"Cast_1:0\", shape=(), dtype=float32)\n",
      "\n",
      "Standarding the process:\n",
      "Tensor(\"truediv_1:0\", shape=(1, 6, 6), dtype=float32)\n",
      "\n",
      "Applying SOFTMAX in the Standarded vlaues, CALCULATING attention weights:\n",
      "Tensor(\"Softmax_1:0\", shape=(1, 6, 6), dtype=float32)\n",
      "\n",
      "Calculating the outpu, multipling the attention weights and the VALUE MATRIX:\n",
      "Tensor(\"MatMul_3:0\", shape=(1, 6, 16), dtype=float32)\n",
      "-------------------------------------------------\n",
      "Applying SELF ATTENTION:\n",
      "-------------------------------------------------\n",
      "\n",
      "multiplying QUERIE and KEY:\n",
      "tf.Tensor(\n",
      "[[[-0.15834472  0.03817163  0.03627749  0.04266986 -0.21614616\n",
      "    0.20030728]\n",
      "  [-0.31579292 -0.12695742 -0.06261243  0.04888483 -0.45044863\n",
      "    0.2571382 ]\n",
      "  [-0.1989534  -0.08700095 -0.04410931  0.02954693 -0.2844577\n",
      "    0.15708253]\n",
      "  [-0.02044782 -0.03128401 -0.01938125 -0.00094726 -0.03136779\n",
      "    0.00048469]\n",
      "  [-0.24992454  0.04086833  0.04437958  0.0638925  -0.34300524\n",
      "    0.3025729 ]\n",
      "  [-0.04778707 -0.13082239 -0.08364684 -0.01250456 -0.0788146\n",
      "   -0.03931691]]], shape=(1, 6, 6), dtype=float32)\n",
      "\n",
      "Taking the dk size:\n",
      "tf.Tensor(16.0, shape=(), dtype=float32)\n",
      "\n",
      "Standarding the process:\n",
      "tf.Tensor(\n",
      "[[[-0.03958618  0.00954291  0.00906937  0.01066747 -0.05403654\n",
      "    0.05007682]\n",
      "  [-0.07894823 -0.03173935 -0.01565311  0.01222121 -0.11261216\n",
      "    0.06428455]\n",
      "  [-0.04973835 -0.02175024 -0.01102733  0.00738673 -0.07111443\n",
      "    0.03927063]\n",
      "  [-0.00511195 -0.007821   -0.00484531 -0.00023682 -0.00784195\n",
      "    0.00012117]\n",
      "  [-0.06248114  0.01021708  0.0110949   0.01597312 -0.08575131\n",
      "    0.07564323]\n",
      "  [-0.01194677 -0.0327056  -0.02091171 -0.00312614 -0.01970365\n",
      "   -0.00982923]]], shape=(1, 6, 6), dtype=float32)\n",
      "\n",
      "Applying SOFTMAX in the Standarded vlaues, CALCULATING attention weights:\n",
      "tf.Tensor(\n",
      "[[[0.16048244 0.16856368 0.16848388 0.16875336 0.15818009 0.17553659]\n",
      "  [0.1579771  0.16561386 0.1682995  0.17305674 0.15274748 0.1823053 ]\n",
      "  [0.1613272  0.16590622 0.16769479 0.17081133 0.15791525 0.17634521]\n",
      "  [0.16652875 0.16607824 0.16657317 0.16734259 0.16607475 0.1674025 ]\n",
      "  [0.15727012 0.1691292  0.16927774 0.17010553 0.15365265 0.18056469]\n",
      "  [0.16739812 0.16395895 0.1659041  0.16888121 0.16610464 0.16775297]]], shape=(1, 6, 6), dtype=float32)\n",
      "\n",
      "Calculating the outpu, multipling the attention weights and the VALUE MATRIX:\n",
      "tf.Tensor(\n",
      "[[[-0.00275537 -0.12209649  0.16723119  0.11057632 -0.06521323\n",
      "    0.10488081  0.1524038  -0.1000996   0.2847925  -0.17741694\n",
      "    0.40864852  0.11811128 -0.19768806  0.35006177  0.1357258\n",
      "    0.08856395]\n",
      "  [-0.00416434 -0.11847653  0.16492574  0.11254904 -0.06892329\n",
      "    0.1043056   0.15419973 -0.10111455  0.28435728 -0.1775904\n",
      "    0.40679842  0.1166482  -0.19403197  0.34835398  0.1387527\n",
      "    0.08469394]\n",
      "  [-0.00275042 -0.12145121  0.16636448  0.11002582 -0.064905\n",
      "    0.10434288  0.15163909 -0.09959627  0.28333908 -0.17651434\n",
      "    0.4065551   0.11750022 -0.19665748  0.34826767  0.13505399\n",
      "    0.08808787]\n",
      "  [-0.00056082 -0.12600377  0.16852073  0.10607372 -0.05865803\n",
      "    0.10435627  0.14761144 -0.09720405  0.28164276 -0.17477392\n",
      "    0.40600577  0.11876906 -0.20063773  0.34798622  0.12927186\n",
      "    0.0933035 ]\n",
      "  [-0.00401157 -0.11965682  0.16622306  0.11298598 -0.06887447\n",
      "    0.10501444  0.15491302 -0.10160285  0.28614652 -0.17865151\n",
      "    0.4095133   0.11754459 -0.19567761  0.35069442  0.13921614\n",
      "    0.08569982]\n",
      "  [-0.00048575 -0.12563387  0.16789535  0.10550366 -0.05820785\n",
      "    0.10392507  0.14686707 -0.09672251  0.2804215  -0.1739932\n",
      "    0.4043082   0.11831997 -0.1999401   0.3465375   0.12854506\n",
      "    0.09309089]]], shape=(1, 6, 16), dtype=float32)\n",
      "-------------------------------------------------\n",
      "Applying SELF ATTENTION:\n",
      "-------------------------------------------------\n",
      "\n",
      "multiplying QUERIE and KEY:\n",
      "tf.Tensor(\n",
      "[[[ 0.02048459 -0.11574282 -0.07832954 -0.02527837  0.01738824\n",
      "   -0.10357634]\n",
      "  [ 0.20639676  0.5510689   0.35199773  0.05151815  0.3390748\n",
      "    0.16002525]\n",
      "  [ 0.13575177  0.37419358  0.23932086  0.03597865  0.2241377\n",
      "    0.11348304]\n",
      "  [ 0.03216441  0.12447903  0.0805077   0.01491179  0.05652436\n",
      "    0.05199397]\n",
      "  [ 0.04812961 -0.10806789 -0.07513386 -0.03017107  0.05649303\n",
      "   -0.12849711]\n",
      "  [ 0.12221185  0.5131044   0.33256844  0.06381536  0.21859983\n",
      "    0.22568619]]], shape=(1, 6, 6), dtype=float32)\n",
      "\n",
      "Taking the dk size:\n",
      "tf.Tensor(16.0, shape=(), dtype=float32)\n",
      "\n",
      "Standarding the process:\n",
      "tf.Tensor(\n",
      "[[[ 0.00512115 -0.0289357  -0.01958239 -0.00631959  0.00434706\n",
      "   -0.02589408]\n",
      "  [ 0.05159919  0.13776723  0.08799943  0.01287954  0.0847687\n",
      "    0.04000631]\n",
      "  [ 0.03393794  0.09354839  0.05983021  0.00899466  0.05603442\n",
      "    0.02837076]\n",
      "  [ 0.0080411   0.03111976  0.02012693  0.00372795  0.01413109\n",
      "    0.01299849]\n",
      "  [ 0.0120324  -0.02701697 -0.01878347 -0.00754277  0.01412326\n",
      "   -0.03212428]\n",
      "  [ 0.03055296  0.1282761   0.08314211  0.01595384  0.05464996\n",
      "    0.05642155]]], shape=(1, 6, 6), dtype=float32)\n",
      "\n",
      "Applying SOFTMAX in the Standarded vlaues, CALCULATING attention weights:\n",
      "tf.Tensor(\n",
      "[[[0.16950798 0.16383226 0.16537184 0.16757974 0.16937682 0.16433133]\n",
      "  [0.16363184 0.17835698 0.1696978  0.15741716 0.16915044 0.16174582]\n",
      "  [0.16447885 0.17458163 0.16879319 0.16042694 0.1681537  0.16356571]\n",
      "  [0.16550043 0.16936438 0.16751277 0.16478814 0.16651139 0.16632292]\n",
      "  [0.17033252 0.16380933 0.16516362 0.16703065 0.17068903 0.16297485]\n",
      "  [0.16147898 0.17805599 0.17019826 0.15913865 0.16541739 0.16571069]]], shape=(1, 6, 6), dtype=float32)\n",
      "\n",
      "Calculating the outpu, multipling the attention weights and the VALUE MATRIX:\n",
      "tf.Tensor(\n",
      "[[[ 3.83179227e-04 -1.27397850e-01  1.68694273e-01  1.03900030e-01\n",
      "   -5.57096153e-02  1.03895359e-01  1.45219490e-01 -9.57407430e-02\n",
      "    2.79653937e-01 -1.73244014e-01  4.03953969e-01  1.18783534e-01\n",
      "   -2.01451927e-01  3.46309006e-01  1.26207560e-01  9.51289162e-02]\n",
      "  [-1.11005822e-04 -1.29925102e-01  1.72933668e-01  1.07728764e-01\n",
      "   -5.87144606e-02  1.06809482e-01  1.50225833e-01 -9.89806578e-02\n",
      "    2.87897348e-01 -1.78509772e-01  4.15423036e-01  1.21826172e-01\n",
      "   -2.06190482e-01  3.56097728e-01  1.31084666e-01  9.65971947e-02]\n",
      "  [-2.66144838e-04 -1.28693119e-01  1.71571866e-01  1.07257478e-01\n",
      "   -5.87490536e-02  1.06062263e-01  1.49463013e-01 -9.84594300e-02\n",
      "    2.86006600e-01 -1.77386448e-01  4.12559599e-01  1.20884627e-01\n",
      "   -2.04466432e-01  3.53629708e-01  1.30580500e-01  9.55508873e-02]\n",
      "  [-5.45809162e-04 -1.26843676e-01  1.69610679e-01  1.06714718e-01\n",
      "   -5.89780360e-02  1.05019994e-01  1.48515940e-01 -9.78018865e-02\n",
      "    2.83419400e-01 -1.75870582e-01  4.08582956e-01  1.19535103e-01\n",
      "   -2.01947391e-01  3.50196689e-01  1.30044848e-01  9.39410999e-02]\n",
      "  [ 7.28143961e-04 -1.28105745e-01  1.69021577e-01  1.03269681e-01\n",
      "   -5.47212362e-02  1.03889816e-01  1.44574195e-01 -9.53567624e-02\n",
      "    2.79366046e-01 -1.72957003e-01  4.03837651e-01  1.18974693e-01\n",
      "   -2.02064201e-01  3.46239120e-01  1.25287220e-01  9.59436744e-02]\n",
      "  [-1.07960438e-03 -1.27854824e-01  1.71904877e-01  1.09430447e-01\n",
      "   -6.14525974e-02  1.06757283e-01  1.51942551e-01 -9.99960750e-02\n",
      "    2.88523048e-01 -1.79202393e-01  4.15486068e-01  1.21212080e-01\n",
      "   -2.04340369e-01  3.56067985e-01  1.33585811e-01  9.42480266e-02]]], shape=(1, 6, 16), dtype=float32)\n",
      "-------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(1, 32), dtype=float32, numpy=\n",
       " array([[ 0.05401925,  0.01050268, -0.10634179, -0.10470515,  0.04372001,\n",
       "          0.1187161 ,  0.01003221,  0.08324832, -0.00135261, -0.06724634,\n",
       "          0.01091498,  0.00532866,  0.05282274, -0.03856877,  0.04868086,\n",
       "         -0.03000465, -0.01532523, -0.10031773, -0.02339172,  0.02514388,\n",
       "          0.09412222,  0.01039118, -0.03518482,  0.06050512, -0.03160544,\n",
       "          0.04403522, -0.03556041,  0.0147457 ,  0.01513099, -0.02689542,\n",
       "         -0.05717932, -0.07184457]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(1, 6, 12), dtype=float32, numpy=\n",
       " array([[[0.16048244, 0.16856368, 0.16848388, 0.16875336, 0.15818009,\n",
       "          0.17553659, 0.16950798, 0.16383226, 0.16537184, 0.16757974,\n",
       "          0.16937682, 0.16433133],\n",
       "         [0.1579771 , 0.16561386, 0.1682995 , 0.17305674, 0.15274748,\n",
       "          0.1823053 , 0.16363184, 0.17835698, 0.1696978 , 0.15741716,\n",
       "          0.16915044, 0.16174582],\n",
       "         [0.1613272 , 0.16590622, 0.16769479, 0.17081133, 0.15791525,\n",
       "          0.17634521, 0.16447885, 0.17458163, 0.16879319, 0.16042694,\n",
       "          0.1681537 , 0.16356571],\n",
       "         [0.16652875, 0.16607824, 0.16657317, 0.16734259, 0.16607475,\n",
       "          0.1674025 , 0.16550043, 0.16936438, 0.16751277, 0.16478814,\n",
       "          0.16651139, 0.16632292],\n",
       "         [0.15727012, 0.1691292 , 0.16927774, 0.17010553, 0.15365265,\n",
       "          0.18056469, 0.17033252, 0.16380933, 0.16516362, 0.16703065,\n",
       "          0.17068903, 0.16297485],\n",
       "         [0.16739812, 0.16395895, 0.1659041 , 0.16888121, 0.16610464,\n",
       "          0.16775297, 0.16147898, 0.17805599, 0.17019826, 0.15913865,\n",
       "          0.16541739, 0.16571069]]], dtype=float32)>)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "teste(y,y,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "\n",
    "class VariableSelectionNetwork(tf.keras.layers.Layer):\n",
    "  def __init__(self, hidden_size, dropout_rate, output_size, input_size=None, additional_context=None, **kwargs):\n",
    "    super(VariableSelectionNetwork, self).__init__(**kwargs)\n",
    "    self.hidden_size = hidden_size\n",
    "    self.input_size = input_size\n",
    "    self.output_size = output_size\n",
    "    self.dropout_rate = dropout_rate\n",
    "    self.additional_context = additional_context\n",
    "\n",
    "    self.flattened_grn = GatedResidualNetwork(\n",
    "        hidden_size, input_size=input_size, output_size=output_size, dropout_rate=dropout_rate, additional_context=additional_context\n",
    "    )\n",
    "    self.per_feature_grn = [GatedResidualNetwork(hidden_size, dropout_rate=dropout_rate) for _ in range(output_size)]\n",
    "\n",
    "  def call(self, inputs):\n",
    "    # Non-static inputs\n",
    "    if self.additional_context is not None:\n",
    "      embedding, static_context = inputs\n",
    "      time_steps = tf.shape(embedding)[1]\n",
    "\n",
    "      flatten = tf.reshape(embedding, (-1, time_steps, self.hidden_size * self.output_size))\n",
    "      static_context = tf.expand_dims(static_context, axis=1)\n",
    "\n",
    "      # Nonlinear transformation with gated residual network\n",
    "      mlp_outputs = self.flattened_grn((flatten, static_context))\n",
    "      sparse_weights = tf.nn.softmax(mlp_outputs, axis=-1)\n",
    "      sparse_weights = tf.expand_dims(sparse_weights, axis=2)\n",
    "\n",
    "      trans_emb_list = []\n",
    "      for i in range(self.output_size):\n",
    "        e = self.per_feature_grn[i](embedding[..., i])  # Select i-th feature\n",
    "        trans_emb_list.append(e)\n",
    "      transformed_embedding = tf.concat(trans_emb_list, axis=-1)\n",
    "\n",
    "      combined = sparse_weights * transformed_embedding\n",
    "      temporal_ctx = tf.reduce_sum(combined, axis=-1)\n",
    "\n",
    "    # Static inputs\n",
    "    else:\n",
    "      embedding = inputs\n",
    "      flatten = tf.reshape(embedding, [-1, self.input_size * self.hidden_size])  # Assuming batch_size is the first dimension\n",
    "\n",
    "      # Nonlinear transformation with gated residual network\n",
    "      mlp_outputs = self.flattened_grn(flatten)\n",
    "      sparse_weights = tf.nn.softmax(mlp_outputs, axis=-1)\n",
    "      sparse_weights = tf.expand_dims(sparse_weights, axis=-1)\n",
    "\n",
    "      trans_emb_list = []\n",
    "      for i in range(self.output_size):\n",
    "        e = self.per_feature_grn[i](embedding[:, i:i + 1, :])  # Select i-th feature channel\n",
    "        trans_emb_list.append(e)\n",
    "      transformed_embedding = tf.concat(trans_emb_list, axis=1)\n",
    "\n",
    "      combined = sparse_weights * transformed_embedding\n",
    "      temporal_ctx = tf.reduce_sum(combined, axis=1)\n",
    "\n",
    "    return temporal_ctx, sparse_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow-park",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.-1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
