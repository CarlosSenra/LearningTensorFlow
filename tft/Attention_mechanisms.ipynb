{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-06 07:59:55.074543: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-06-06 07:59:55.394754: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-06 07:59:56.791241: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(q, k, v, mask):\n",
    "  print(\"Applying SELF ATTENTION:\")\n",
    "  print(\"-------------------------------------------------\")\n",
    "  \n",
    "  matmul_qk = tf.matmul(q, k, transpose_b=True) \n",
    "  print(\"\\nmultiplying QUERIE and KEY:\")\n",
    "  print(matmul_qk)\n",
    "\n",
    "  dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "  print(\"\\nTaking the dk size:\")\n",
    "  print(dk)\n",
    "\n",
    "  scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "  print(\"\\nStandarding the process:\")\n",
    "  print(scaled_attention_logits)\n",
    "\n",
    "  if mask is not None:\n",
    "    scaled_attention_logits += (mask * -1e9)\n",
    "\n",
    "  attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  \n",
    "  print(\"\\nApplying SOFTMAX in the Standarded vlaues, CALCULATING attention weights:\")\n",
    "  print(attention_weights)\n",
    "  output = tf.matmul(attention_weights, v)\n",
    "  print(\"\\nCalculating the outpu, multipling the attention weights and the VALUE MATRIX:\")\n",
    "  print(output)\n",
    "  print(\"-------------------------------------------------\")\n",
    "  return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 6, 2), dtype=float32, numpy=\n",
       "array([[[0.86278236, 0.30886674],\n",
       "        [0.87680805, 0.03432608],\n",
       "        [0.47888672, 0.74300647],\n",
       "        [0.8201885 , 0.4476421 ],\n",
       "        [0.5970541 , 0.7578428 ],\n",
       "        [0.68740153, 0.8398106 ]]], dtype=float32)>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = tf.random.uniform((1, 6, 2))  # (batch_size, encoder_sequence, d_model)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying SELF ATTENTION:\n",
      "-------------------------------------------------\n",
      "\n",
      "multiplying QUERIE and KEY:\n",
      "tf.Tensor(\n",
      "[[[0.8397921  0.7670967  0.642665   0.8459059  0.7492002  0.8524675 ]\n",
      "  [0.7670967  0.7699706  0.44539624 0.7345137  0.54951566 0.6315466 ]\n",
      "  [0.642665   0.44539624 0.7813911  0.72537833 0.8490034  0.9531722 ]\n",
      "  [0.8459059  0.7345137  0.72537833 0.87309265 0.82893926 0.93973345]\n",
      "  [0.7492002  0.54951566 0.8490034  0.82893926 0.9307993  1.0468603 ]\n",
      "  [0.8524675  0.6315466  0.9531722  0.93973345 1.0468603  1.1778027 ]]], shape=(1, 6, 6), dtype=float32)\n",
      "\n",
      "Taking the dk size:\n",
      "tf.Tensor(2.0, shape=(), dtype=float32)\n",
      "\n",
      "Standarding the process:\n",
      "tf.Tensor(\n",
      "[[[0.59382266 0.5424193  0.45443282 0.5981458  0.5297646  0.6027855 ]\n",
      "  [0.5424193  0.5444514  0.31494272 0.5193796  0.38856626 0.4465709 ]\n",
      "  [0.45443282 0.31494272 0.55252695 0.51291996 0.6003361  0.67399454]\n",
      "  [0.5981458  0.5193796  0.51291996 0.6173698  0.58614856 0.6644919 ]\n",
      "  [0.5297646  0.38856626 0.6003361  0.58614856 0.6581745  0.74024206]\n",
      "  [0.6027855  0.4465709  0.67399454 0.6644919  0.74024206 0.8328323 ]]], shape=(1, 6, 6), dtype=float32)\n",
      "\n",
      "Applying SOFTMAX in the Standarded vlaues, CALCULATING attention weights:\n",
      "tf.Tensor(\n",
      "[[[0.17327806 0.16459607 0.15073267 0.1740288  0.16252626 0.17483813]\n",
      "  [0.18044682 0.18081388 0.14373331 0.17633691 0.15471473 0.16395429]\n",
      "  [0.1553829  0.1351523  0.17139767 0.16474181 0.17979111 0.19353418]\n",
      "  [0.16895768 0.15616016 0.15515466 0.17223714 0.16694278 0.1805476 ]\n",
      "  [0.15696792 0.13629793 0.1684456  0.16607265 0.17847551 0.19374035]\n",
      "  [0.15627827 0.13367662 0.16781248 0.16622537 0.17930616 0.19670102]]], shape=(1, 6, 6), dtype=float32)\n",
      "\n",
      "Calculating the outpu, multipling the attention weights and the VALUE MATRIX:\n",
      "tf.Tensor(\n",
      "[[[0.7259617  0.519068  ]\n",
      "  [0.7327624  0.5026113 ]\n",
      "  [0.7101444  0.5525122 ]\n",
      "  [0.7220472  0.52806944]\n",
      "  [0.7115505  0.5506196 ]\n",
      "  [0.7110103  0.5530305 ]]], shape=(1, 6, 2), dtype=float32)\n",
      "-------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(1, 6, 2), dtype=float32, numpy=\n",
       " array([[[0.7259617 , 0.519068  ],\n",
       "         [0.7327624 , 0.5026113 ],\n",
       "         [0.7101444 , 0.5525122 ],\n",
       "         [0.7220472 , 0.52806944],\n",
       "         [0.7115505 , 0.5506196 ],\n",
       "         [0.7110103 , 0.5530305 ]]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(1, 6, 6), dtype=float32, numpy=\n",
       " array([[[0.17327806, 0.16459607, 0.15073267, 0.1740288 , 0.16252626,\n",
       "          0.17483813],\n",
       "         [0.18044682, 0.18081388, 0.14373331, 0.17633691, 0.15471473,\n",
       "          0.16395429],\n",
       "         [0.1553829 , 0.1351523 , 0.17139767, 0.16474181, 0.17979111,\n",
       "          0.19353418],\n",
       "         [0.16895768, 0.15616016, 0.15515466, 0.17223714, 0.16694278,\n",
       "          0.1805476 ],\n",
       "         [0.15696792, 0.13629793, 0.1684456 , 0.16607265, 0.17847551,\n",
       "          0.19374035],\n",
       "         [0.15627827, 0.13367662, 0.16781248, 0.16622537, 0.17930616,\n",
       "          0.19670102]]], dtype=float32)>)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled_dot_product_attention(y, y, y, mask=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "  def __init__(self, d_model, num_heads):\n",
    "    super(MultiHeadAttention, self).__init__()\n",
    "    self.num_heads = num_heads\n",
    "    self.d_model = d_model\n",
    "\n",
    "    assert d_model % self.num_heads == 0\n",
    "\n",
    "    self.depth = d_model // self.num_heads\n",
    "\n",
    "    self.wq = tf.keras.layers.Dense(d_model,use_bias=False)\n",
    "    self.wk = tf.keras.layers.Dense(d_model,use_bias=False)\n",
    "    self.wv = tf.keras.layers.Dense(d_model,use_bias=False)\n",
    "\n",
    "    self.dense = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "  def split_heads(self, x, batch_size):\n",
    "    \"\"\"Split the last dimension into (num_heads, depth).\n",
    "    Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
    "    \"\"\"\n",
    "    x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "    return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "  def call(self, v, k, q, mask):\n",
    "    batch_size = tf.shape(q)[0]\n",
    "    print(\"Batch size = :{}\".format(batch_size))\n",
    "    q = self.wq(q)\n",
    "    k = self.wk(k)  \n",
    "    v = self.wv(v)\n",
    "    print(\"Querie : \\n\")  \n",
    "    print(q)\n",
    "    print(\"\\n Key : \\n\")  \n",
    "    print(k)\n",
    "    print(\"\\n Value : \\n\")  \n",
    "    print(v)\n",
    "\n",
    "    q = self.split_heads(q, batch_size) \n",
    "    k = self.split_heads(k, batch_size)  \n",
    "    v = self.split_heads(v, batch_size)\n",
    "\n",
    "    print(\"\\n Querie splited: \\n\")  \n",
    "    print(q)  \n",
    "    print(\"\\n Key splited: \\n\")  \n",
    "    print(k)\n",
    "    print(\"\\n Value splited: \\n\")  \n",
    "    print(v)  \n",
    "\n",
    "    scaled_attention, attention_weights = scaled_dot_product_attention(\n",
    "        q, k, v, mask)\n",
    "\n",
    "    scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3]) \n",
    "    print(\"\\n Transposing the output self attention \\n\")\n",
    "    print(scaled_attention)\n",
    "    concat_attention = tf.reshape(scaled_attention,\n",
    "                                  (batch_size, -1, self.d_model))  \n",
    "    print(\"\\n Concating the Transposed output self attention \\n\")\n",
    "    print(concat_attention)\n",
    "    output = self.dense(concat_attention)  \n",
    "    print(\"\\n Applying into a dense layer the Concat Transposed output self attention \\n\")\n",
    "    print(output)\n",
    "    return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_mha = MultiHeadAttention(d_model=32, num_heads=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size = :Tensor(\"strided_slice:0\", shape=(), dtype=int32)\n",
      "Querie : \n",
      "\n",
      "Tensor(\"dense_64_1/Add:0\", shape=(1, 6, 32), dtype=float32)\n",
      "\n",
      " Key : \n",
      "\n",
      "Tensor(\"dense_65_1/Add:0\", shape=(1, 6, 32), dtype=float32)\n",
      "\n",
      " Value : \n",
      "\n",
      "Tensor(\"dense_66_1/Add:0\", shape=(1, 6, 32), dtype=float32)\n",
      "\n",
      " Querie splited: \n",
      "\n",
      "Tensor(\"transpose:0\", shape=(1, 2, 6, 16), dtype=float32)\n",
      "\n",
      " Key splited: \n",
      "\n",
      "Tensor(\"transpose_1:0\", shape=(1, 2, 6, 16), dtype=float32)\n",
      "\n",
      " Value splited: \n",
      "\n",
      "Tensor(\"transpose_2:0\", shape=(1, 2, 6, 16), dtype=float32)\n",
      "Applying SELF ATTENTION:\n",
      "-------------------------------------------------\n",
      "\n",
      "multiplying QUERIE and KEY:\n",
      "Tensor(\"MatMul:0\", shape=(1, 2, 6, 6), dtype=float32)\n",
      "\n",
      "Taking the dk size:\n",
      "Tensor(\"Cast:0\", shape=(), dtype=float32)\n",
      "\n",
      "Standarding the process:\n",
      "Tensor(\"truediv:0\", shape=(1, 2, 6, 6), dtype=float32)\n",
      "\n",
      "Applying SOFTMAX in the Standarded vlaues, CALCULATING attention weights:\n",
      "Tensor(\"Softmax:0\", shape=(1, 2, 6, 6), dtype=float32)\n",
      "\n",
      "Calculating the outpu, multipling the attention weights and the VALUE MATRIX:\n",
      "Tensor(\"MatMul_1:0\", shape=(1, 2, 6, 16), dtype=float32)\n",
      "-------------------------------------------------\n",
      "\n",
      " Transposing the output self attention \n",
      "\n",
      "Tensor(\"transpose_3:0\", shape=(1, 6, 2, 16), dtype=float32)\n",
      "\n",
      " Concating the Transposed output self attention \n",
      "\n",
      "Tensor(\"Reshape_3:0\", shape=(1, 6, 32), dtype=float32)\n",
      "\n",
      " Applying into a dense layer the Concat Transposed output self attention \n",
      "\n",
      "Tensor(\"dense_67_1/Add:0\", shape=(1, 6, 32), dtype=float32)\n",
      "Batch size = :1\n",
      "Querie : \n",
      "\n",
      "tf.Tensor(\n",
      "[[[-2.08570480e-01  3.42350364e-01  1.21355042e-01 -3.73673290e-02\n",
      "    1.41014561e-01  9.23328623e-02 -5.12372628e-02 -3.92324895e-01\n",
      "   -9.00749564e-02  8.84915665e-02  1.37724683e-01 -4.96972464e-02\n",
      "   -2.58797437e-01  2.70375937e-01 -1.65001154e-01 -8.21716115e-02\n",
      "    3.84102732e-01 -1.28081709e-01 -1.97400123e-01 -2.77033120e-01\n",
      "   -2.96242774e-01 -1.04612902e-01 -1.13805197e-01 -7.42449462e-02\n",
      "    1.54046252e-01  7.34525472e-02 -2.89378047e-01 -1.67475529e-02\n",
      "   -4.56960142e-01  5.04958667e-02 -2.42182388e-04  1.25581309e-01]\n",
      "  [-3.27894390e-01  3.54409724e-01  5.88832423e-02  3.26168467e-03\n",
      "    2.46616393e-01  1.25728510e-02  5.51335476e-02 -3.31463039e-01\n",
      "   -5.80427982e-02  6.84079602e-02  6.49089962e-02 -9.55300704e-02\n",
      "   -2.03875363e-01  2.61015505e-01 -2.20662504e-01 -1.85843796e-01\n",
      "    2.82236069e-01 -4.43205237e-02 -3.13346118e-01 -3.22993249e-01\n",
      "   -2.30019450e-01 -8.74838680e-02 -5.03891520e-02 -8.20213631e-02\n",
      "    2.48727769e-01  3.53449583e-02 -2.72724956e-01 -8.60310122e-02\n",
      "   -3.76555920e-01 -5.87767810e-02 -2.21304037e-02  1.84096843e-01]\n",
      "  [ 1.21261366e-01  1.76744163e-01  1.99116454e-01 -1.05049603e-01\n",
      "   -1.32948473e-01  2.17389241e-01 -2.47619674e-01 -3.55232596e-01\n",
      "   -1.18480287e-01  9.31197405e-02  2.29894787e-01  6.44700527e-02\n",
      "   -2.64536262e-01  1.78195938e-01  1.67330410e-02  1.63619727e-01\n",
      "    4.34230894e-01 -2.46600270e-01  1.20926522e-01 -6.90081418e-02\n",
      "   -3.09670538e-01 -9.65629742e-02 -1.96605504e-01 -2.77781878e-02\n",
      "   -1.02955349e-01  1.21122912e-01 -2.04284638e-01  1.31799236e-01\n",
      "   -4.33211416e-01  2.53116399e-01  4.46084850e-02 -4.57586385e-02]\n",
      "  [-1.34400755e-01  3.21871340e-01  1.50869504e-01 -5.82416579e-02\n",
      "    7.71350116e-02  1.32545024e-01 -1.07771285e-01 -4.10002023e-01\n",
      "   -1.04082882e-01  9.59804803e-02  1.72276542e-01 -2.24374793e-02\n",
      "   -2.78598130e-01  2.64606714e-01 -1.27666786e-01 -2.17331182e-02\n",
      "    4.24703598e-01 -1.69053629e-01 -1.25542834e-01 -2.40516201e-01\n",
      "   -3.20756644e-01 -1.09822504e-01 -1.44144982e-01 -6.69602007e-02\n",
      "    9.56565365e-02  9.14794430e-02 -2.86858767e-01  2.21007019e-02\n",
      "   -4.82792050e-01  1.08658604e-01  1.18268365e-02  8.82674456e-02]\n",
      "  [ 8.13051686e-02  2.24270776e-01  2.09405795e-01 -1.06116071e-01\n",
      "   -1.03485063e-01  2.22051501e-01 -2.44104639e-01 -4.02359605e-01\n",
      "   -1.27526075e-01  1.03125110e-01  2.41383716e-01  5.32398783e-02\n",
      "   -2.94172049e-01  2.13875383e-01 -1.10707274e-02  1.42311096e-01\n",
      "    4.76216286e-01 -2.55708516e-01  8.28142762e-02 -1.11023843e-01\n",
      "   -3.43264729e-01 -1.09040871e-01 -2.05780104e-01 -3.85922715e-02\n",
      "   -7.28008151e-02  1.27321735e-01 -2.41819784e-01  1.22725278e-01\n",
      "   -4.87167686e-01  2.49215901e-01  4.24252301e-02 -2.30104513e-02]\n",
      "  [ 8.00435245e-02  2.58967727e-01  2.33553082e-01 -1.17348842e-01\n",
      "   -1.07056789e-01  2.46144742e-01 -2.68499523e-01 -4.55377996e-01\n",
      "   -1.42904282e-01  1.16211981e-01  2.69128531e-01  5.60280494e-02\n",
      "   -3.31768274e-01  2.44629905e-01 -1.89448502e-02  1.51871920e-01\n",
      "    5.35628617e-01 -2.84358650e-01  8.21549818e-02 -1.32674858e-01\n",
      "   -3.86896223e-01 -1.23337962e-01 -2.29282618e-01 -4.52008024e-02\n",
      "   -7.30318725e-02  1.41989768e-01 -2.75913477e-01  1.33221582e-01\n",
      "   -5.50609887e-01  2.74046123e-01  4.62845042e-02 -1.98846329e-02]]], shape=(1, 6, 32), dtype=float32)\n",
      "\n",
      " Key : \n",
      "\n",
      "tf.Tensor(\n",
      "[[[-0.18470035  0.11951533 -0.2523023   0.2495941  -0.16439089\n",
      "    0.3320933  -0.19681858  0.23661569 -0.0810025   0.10902608\n",
      "    0.30071276  0.11708196  0.3101949  -0.04237411 -0.1652894\n",
      "   -0.31318545 -0.28254727 -0.19907115 -0.30384827  0.01443151\n",
      "    0.19317546 -0.05411104  0.16377075 -0.2519652  -0.18436289\n",
      "   -0.07364836 -0.1539557   0.10999507 -0.15383215  0.2802323\n",
      "    0.14493522  0.21458575]\n",
      "  [-0.29986614  0.06223213 -0.1401742   0.2984965  -0.18707018\n",
      "    0.34830222 -0.08957372  0.32330105  0.02654659  0.08173066\n",
      "    0.2670362   0.11157002  0.27109396 -0.02037003 -0.2192196\n",
      "   -0.3178501  -0.34189615 -0.0872613  -0.3483118   0.01122752\n",
      "    0.18407008 -0.12472035  0.25082123 -0.16509128 -0.08423886\n",
      "   -0.04291537 -0.14847471  0.18577182 -0.1066241   0.19983986\n",
      "    0.23885265  0.3130128 ]\n",
      "  [ 0.12680249  0.18742597 -0.37767422  0.04685059 -0.05034054\n",
      "    0.16222629 -0.33505008 -0.03803254 -0.26753914  0.11994459\n",
      "    0.24575762  0.08014692  0.262426   -0.06991591  0.0130241\n",
      "   -0.17470555 -0.04487838 -0.34570846 -0.08784284  0.01504047\n",
      "    0.13225853  0.11252955 -0.0816327  -0.3258432  -0.3131639\n",
      "   -0.1061606  -0.10177598 -0.09021875 -0.18701519  0.3292208\n",
      "   -0.10675299 -0.07499855]\n",
      "  [-0.11378614  0.1462455  -0.3038829   0.21256503 -0.14525254\n",
      "    0.30974263 -0.24795096  0.17929469 -0.13698281  0.11965847\n",
      "    0.3071145   0.11538726  0.31920195 -0.05278476 -0.12889716\n",
      "   -0.29795915 -0.238431   -0.25262758 -0.2670722   0.01561353\n",
      "    0.1903855  -0.0130224   0.10919236 -0.28964573 -0.23207545\n",
      "   -0.08760435 -0.15075384  0.06380107 -0.17362465  0.31319955\n",
      "    0.0873347   0.15168598]\n",
      "  [ 0.09048598  0.19797605 -0.40081045  0.08544113 -0.07482129\n",
      "    0.20877215 -0.35115558  0.0025133  -0.26793748  0.13202105\n",
      "    0.28315458  0.09545402  0.30057353 -0.07348997 -0.01464862\n",
      "   -0.21755779 -0.08895592 -0.3616704  -0.13334133  0.01667919\n",
      "    0.1575129   0.09826767 -0.05091157 -0.35141495 -0.32828298\n",
      "   -0.11311046 -0.12207749 -0.0678845  -0.20320038  0.35925567\n",
      "   -0.07790683 -0.03628118]\n",
      "  [ 0.09105471  0.22100437 -0.4478623   0.10361739 -0.08848432\n",
      "    0.24162884 -0.39137048  0.01258627 -0.29574433  0.14859763\n",
      "    0.32148972  0.10903069  0.34089187 -0.0819554  -0.02286104\n",
      "   -0.25042915 -0.10882367 -0.40293798 -0.15814334  0.01880078\n",
      "    0.17991523  0.10497893 -0.04874168 -0.39394772 -0.3658936\n",
      "   -0.12649052 -0.13961636 -0.0694998  -0.22813283  0.40367955\n",
      "   -0.07898261 -0.03066292]]], shape=(1, 6, 32), dtype=float32)\n",
      "\n",
      " Value : \n",
      "\n",
      "tf.Tensor(\n",
      "[[[ 0.35725102 -0.30849162 -0.36703798  0.04390346 -0.4403824\n",
      "   -0.38757056 -0.01654997 -0.22568141 -0.18169898 -0.11896265\n",
      "    0.09764928  0.09028801 -0.3910891  -0.11817709 -0.36837086\n",
      "   -0.30739883  0.40125304  0.34403268  0.46363878  0.25463778\n",
      "   -0.21093191  0.01264321 -0.46264696 -0.13225107  0.0751141\n",
      "    0.2654009   0.09108387 -0.20330328 -0.4108426  -0.15846542\n",
      "    0.02053238 -0.26081595]\n",
      "  [ 0.30228722 -0.35811672 -0.32067072 -0.02309717 -0.33549434\n",
      "   -0.27667087 -0.00465836 -0.2045559  -0.25215262 -0.21591344\n",
      "    0.16642599  0.12986085 -0.3458025  -0.2246203  -0.34846509\n",
      "   -0.30336297  0.33179897  0.33634323  0.35800448  0.27821976\n",
      "   -0.21979995 -0.02188348 -0.3529857  -0.12634997  0.15108919\n",
      "    0.2950072  -0.01516297 -0.31768957 -0.35494655 -0.10691744\n",
      "   -0.05760159 -0.20459354]\n",
      "  [ 0.32254037 -0.08002163 -0.31072208  0.16281207 -0.4735167\n",
      "   -0.45473924 -0.03404878 -0.17595676  0.03715311  0.12823407\n",
      "   -0.08316986 -0.02779244 -0.3226617   0.14810361 -0.25740516\n",
      "   -0.18908978  0.37805185  0.21811076  0.48872375  0.10158587\n",
      "   -0.10595732  0.0780284  -0.49637353 -0.08986634 -0.11114422\n",
      "    0.0956008   0.2708074   0.11426497 -0.3559735  -0.19861396\n",
      "    0.17182557 -0.26838228]\n",
      "  [ 0.373096   -0.2686842  -0.37775123  0.07904299 -0.4803735\n",
      "   -0.43300802 -0.0224328  -0.22820029 -0.13554008 -0.06074049\n",
      "    0.05581084  0.0648368  -0.40023506 -0.05475689 -0.3644514\n",
      "   -0.2971999   0.42330322  0.33436623  0.50310117  0.23135504\n",
      "   -0.197522    0.03115462 -0.5043681  -0.13015777  0.03022042\n",
      "    0.23836412  0.14593932 -0.13206673 -0.42503548 -0.18046175\n",
      "    0.06275025 -0.28125146]\n",
      "  [ 0.3654991  -0.12665576 -0.35585022  0.16217235 -0.52282345\n",
      "   -0.49630657 -0.03512072 -0.20443028  0.00563574  0.10260564\n",
      "   -0.06319452 -0.01168276 -0.37115163  0.12164889 -0.3053135\n",
      "   -0.23030391  0.42554322  0.2639248   0.5411053   0.13837145\n",
      "   -0.13538112  0.07634766 -0.54822516 -0.10718856 -0.09351212\n",
      "    0.1344352   0.2726983   0.07550688 -0.406095   -0.21499994\n",
      "    0.16692841 -0.29816356]\n",
      "  [ 0.41369653 -0.15104121 -0.4035748   0.17878959 -0.58882797\n",
      "   -0.5576955  -0.03901238 -0.232464   -0.00140935  0.10701457\n",
      "   -0.06489567 -0.0089921  -0.42127234  0.12782733 -0.34848437\n",
      "   -0.26409703  0.48104748  0.30230832  0.6097446   0.16158499\n",
      "   -0.15650365  0.08383685 -0.6174726  -0.12246653 -0.09891587\n",
      "    0.1577375   0.3013588   0.07393555 -0.46022448 -0.24120131\n",
      "    0.18300715 -0.3362078 ]]], shape=(1, 6, 32), dtype=float32)\n",
      "\n",
      " Querie splited: \n",
      "\n",
      "tf.Tensor(\n",
      "[[[[-2.08570480e-01  3.42350364e-01  1.21355042e-01 -3.73673290e-02\n",
      "     1.41014561e-01  9.23328623e-02 -5.12372628e-02 -3.92324895e-01\n",
      "    -9.00749564e-02  8.84915665e-02  1.37724683e-01 -4.96972464e-02\n",
      "    -2.58797437e-01  2.70375937e-01 -1.65001154e-01 -8.21716115e-02]\n",
      "   [-3.27894390e-01  3.54409724e-01  5.88832423e-02  3.26168467e-03\n",
      "     2.46616393e-01  1.25728510e-02  5.51335476e-02 -3.31463039e-01\n",
      "    -5.80427982e-02  6.84079602e-02  6.49089962e-02 -9.55300704e-02\n",
      "    -2.03875363e-01  2.61015505e-01 -2.20662504e-01 -1.85843796e-01]\n",
      "   [ 1.21261366e-01  1.76744163e-01  1.99116454e-01 -1.05049603e-01\n",
      "    -1.32948473e-01  2.17389241e-01 -2.47619674e-01 -3.55232596e-01\n",
      "    -1.18480287e-01  9.31197405e-02  2.29894787e-01  6.44700527e-02\n",
      "    -2.64536262e-01  1.78195938e-01  1.67330410e-02  1.63619727e-01]\n",
      "   [-1.34400755e-01  3.21871340e-01  1.50869504e-01 -5.82416579e-02\n",
      "     7.71350116e-02  1.32545024e-01 -1.07771285e-01 -4.10002023e-01\n",
      "    -1.04082882e-01  9.59804803e-02  1.72276542e-01 -2.24374793e-02\n",
      "    -2.78598130e-01  2.64606714e-01 -1.27666786e-01 -2.17331182e-02]\n",
      "   [ 8.13051686e-02  2.24270776e-01  2.09405795e-01 -1.06116071e-01\n",
      "    -1.03485063e-01  2.22051501e-01 -2.44104639e-01 -4.02359605e-01\n",
      "    -1.27526075e-01  1.03125110e-01  2.41383716e-01  5.32398783e-02\n",
      "    -2.94172049e-01  2.13875383e-01 -1.10707274e-02  1.42311096e-01]\n",
      "   [ 8.00435245e-02  2.58967727e-01  2.33553082e-01 -1.17348842e-01\n",
      "    -1.07056789e-01  2.46144742e-01 -2.68499523e-01 -4.55377996e-01\n",
      "    -1.42904282e-01  1.16211981e-01  2.69128531e-01  5.60280494e-02\n",
      "    -3.31768274e-01  2.44629905e-01 -1.89448502e-02  1.51871920e-01]]\n",
      "\n",
      "  [[ 3.84102732e-01 -1.28081709e-01 -1.97400123e-01 -2.77033120e-01\n",
      "    -2.96242774e-01 -1.04612902e-01 -1.13805197e-01 -7.42449462e-02\n",
      "     1.54046252e-01  7.34525472e-02 -2.89378047e-01 -1.67475529e-02\n",
      "    -4.56960142e-01  5.04958667e-02 -2.42182388e-04  1.25581309e-01]\n",
      "   [ 2.82236069e-01 -4.43205237e-02 -3.13346118e-01 -3.22993249e-01\n",
      "    -2.30019450e-01 -8.74838680e-02 -5.03891520e-02 -8.20213631e-02\n",
      "     2.48727769e-01  3.53449583e-02 -2.72724956e-01 -8.60310122e-02\n",
      "    -3.76555920e-01 -5.87767810e-02 -2.21304037e-02  1.84096843e-01]\n",
      "   [ 4.34230894e-01 -2.46600270e-01  1.20926522e-01 -6.90081418e-02\n",
      "    -3.09670538e-01 -9.65629742e-02 -1.96605504e-01 -2.77781878e-02\n",
      "    -1.02955349e-01  1.21122912e-01 -2.04284638e-01  1.31799236e-01\n",
      "    -4.33211416e-01  2.53116399e-01  4.46084850e-02 -4.57586385e-02]\n",
      "   [ 4.24703598e-01 -1.69053629e-01 -1.25542834e-01 -2.40516201e-01\n",
      "    -3.20756644e-01 -1.09822504e-01 -1.44144982e-01 -6.69602007e-02\n",
      "     9.56565365e-02  9.14794430e-02 -2.86858767e-01  2.21007019e-02\n",
      "    -4.82792050e-01  1.08658604e-01  1.18268365e-02  8.82674456e-02]\n",
      "   [ 4.76216286e-01 -2.55708516e-01  8.28142762e-02 -1.11023843e-01\n",
      "    -3.43264729e-01 -1.09040871e-01 -2.05780104e-01 -3.85922715e-02\n",
      "    -7.28008151e-02  1.27321735e-01 -2.41819784e-01  1.22725278e-01\n",
      "    -4.87167686e-01  2.49215901e-01  4.24252301e-02 -2.30104513e-02]\n",
      "   [ 5.35628617e-01 -2.84358650e-01  8.21549818e-02 -1.32674858e-01\n",
      "    -3.86896223e-01 -1.23337962e-01 -2.29282618e-01 -4.52008024e-02\n",
      "    -7.30318725e-02  1.41989768e-01 -2.75913477e-01  1.33221582e-01\n",
      "    -5.50609887e-01  2.74046123e-01  4.62845042e-02 -1.98846329e-02]]]], shape=(1, 2, 6, 16), dtype=float32)\n",
      "\n",
      " Key splited: \n",
      "\n",
      "tf.Tensor(\n",
      "[[[[-0.18470035  0.11951533 -0.2523023   0.2495941  -0.16439089\n",
      "     0.3320933  -0.19681858  0.23661569 -0.0810025   0.10902608\n",
      "     0.30071276  0.11708196  0.3101949  -0.04237411 -0.1652894\n",
      "    -0.31318545]\n",
      "   [-0.29986614  0.06223213 -0.1401742   0.2984965  -0.18707018\n",
      "     0.34830222 -0.08957372  0.32330105  0.02654659  0.08173066\n",
      "     0.2670362   0.11157002  0.27109396 -0.02037003 -0.2192196\n",
      "    -0.3178501 ]\n",
      "   [ 0.12680249  0.18742597 -0.37767422  0.04685059 -0.05034054\n",
      "     0.16222629 -0.33505008 -0.03803254 -0.26753914  0.11994459\n",
      "     0.24575762  0.08014692  0.262426   -0.06991591  0.0130241\n",
      "    -0.17470555]\n",
      "   [-0.11378614  0.1462455  -0.3038829   0.21256503 -0.14525254\n",
      "     0.30974263 -0.24795096  0.17929469 -0.13698281  0.11965847\n",
      "     0.3071145   0.11538726  0.31920195 -0.05278476 -0.12889716\n",
      "    -0.29795915]\n",
      "   [ 0.09048598  0.19797605 -0.40081045  0.08544113 -0.07482129\n",
      "     0.20877215 -0.35115558  0.0025133  -0.26793748  0.13202105\n",
      "     0.28315458  0.09545402  0.30057353 -0.07348997 -0.01464862\n",
      "    -0.21755779]\n",
      "   [ 0.09105471  0.22100437 -0.4478623   0.10361739 -0.08848432\n",
      "     0.24162884 -0.39137048  0.01258627 -0.29574433  0.14859763\n",
      "     0.32148972  0.10903069  0.34089187 -0.0819554  -0.02286104\n",
      "    -0.25042915]]\n",
      "\n",
      "  [[-0.28254727 -0.19907115 -0.30384827  0.01443151  0.19317546\n",
      "    -0.05411104  0.16377075 -0.2519652  -0.18436289 -0.07364836\n",
      "    -0.1539557   0.10999507 -0.15383215  0.2802323   0.14493522\n",
      "     0.21458575]\n",
      "   [-0.34189615 -0.0872613  -0.3483118   0.01122752  0.18407008\n",
      "    -0.12472035  0.25082123 -0.16509128 -0.08423886 -0.04291537\n",
      "    -0.14847471  0.18577182 -0.1066241   0.19983986  0.23885265\n",
      "     0.3130128 ]\n",
      "   [-0.04487838 -0.34570846 -0.08784284  0.01504047  0.13225853\n",
      "     0.11252955 -0.0816327  -0.3258432  -0.3131639  -0.1061606\n",
      "    -0.10177598 -0.09021875 -0.18701519  0.3292208  -0.10675299\n",
      "    -0.07499855]\n",
      "   [-0.238431   -0.25262758 -0.2670722   0.01561353  0.1903855\n",
      "    -0.0130224   0.10919236 -0.28964573 -0.23207545 -0.08760435\n",
      "    -0.15075384  0.06380107 -0.17362465  0.31319955  0.0873347\n",
      "     0.15168598]\n",
      "   [-0.08895592 -0.3616704  -0.13334133  0.01667919  0.1575129\n",
      "     0.09826767 -0.05091157 -0.35141495 -0.32828298 -0.11311046\n",
      "    -0.12207749 -0.0678845  -0.20320038  0.35925567 -0.07790683\n",
      "    -0.03628118]\n",
      "   [-0.10882367 -0.40293798 -0.15814334  0.01880078  0.17991523\n",
      "     0.10497893 -0.04874168 -0.39394772 -0.3658936  -0.12649052\n",
      "    -0.13961636 -0.0694998  -0.22813283  0.40367955 -0.07898261\n",
      "    -0.03066292]]]], shape=(1, 2, 6, 16), dtype=float32)\n",
      "\n",
      " Value splited: \n",
      "\n",
      "tf.Tensor(\n",
      "[[[[ 0.35725102 -0.30849162 -0.36703798  0.04390346 -0.4403824\n",
      "    -0.38757056 -0.01654997 -0.22568141 -0.18169898 -0.11896265\n",
      "     0.09764928  0.09028801 -0.3910891  -0.11817709 -0.36837086\n",
      "    -0.30739883]\n",
      "   [ 0.30228722 -0.35811672 -0.32067072 -0.02309717 -0.33549434\n",
      "    -0.27667087 -0.00465836 -0.2045559  -0.25215262 -0.21591344\n",
      "     0.16642599  0.12986085 -0.3458025  -0.2246203  -0.34846509\n",
      "    -0.30336297]\n",
      "   [ 0.32254037 -0.08002163 -0.31072208  0.16281207 -0.4735167\n",
      "    -0.45473924 -0.03404878 -0.17595676  0.03715311  0.12823407\n",
      "    -0.08316986 -0.02779244 -0.3226617   0.14810361 -0.25740516\n",
      "    -0.18908978]\n",
      "   [ 0.373096   -0.2686842  -0.37775123  0.07904299 -0.4803735\n",
      "    -0.43300802 -0.0224328  -0.22820029 -0.13554008 -0.06074049\n",
      "     0.05581084  0.0648368  -0.40023506 -0.05475689 -0.3644514\n",
      "    -0.2971999 ]\n",
      "   [ 0.3654991  -0.12665576 -0.35585022  0.16217235 -0.52282345\n",
      "    -0.49630657 -0.03512072 -0.20443028  0.00563574  0.10260564\n",
      "    -0.06319452 -0.01168276 -0.37115163  0.12164889 -0.3053135\n",
      "    -0.23030391]\n",
      "   [ 0.41369653 -0.15104121 -0.4035748   0.17878959 -0.58882797\n",
      "    -0.5576955  -0.03901238 -0.232464   -0.00140935  0.10701457\n",
      "    -0.06489567 -0.0089921  -0.42127234  0.12782733 -0.34848437\n",
      "    -0.26409703]]\n",
      "\n",
      "  [[ 0.40125304  0.34403268  0.46363878  0.25463778 -0.21093191\n",
      "     0.01264321 -0.46264696 -0.13225107  0.0751141   0.2654009\n",
      "     0.09108387 -0.20330328 -0.4108426  -0.15846542  0.02053238\n",
      "    -0.26081595]\n",
      "   [ 0.33179897  0.33634323  0.35800448  0.27821976 -0.21979995\n",
      "    -0.02188348 -0.3529857  -0.12634997  0.15108919  0.2950072\n",
      "    -0.01516297 -0.31768957 -0.35494655 -0.10691744 -0.05760159\n",
      "    -0.20459354]\n",
      "   [ 0.37805185  0.21811076  0.48872375  0.10158587 -0.10595732\n",
      "     0.0780284  -0.49637353 -0.08986634 -0.11114422  0.0956008\n",
      "     0.2708074   0.11426497 -0.3559735  -0.19861396  0.17182557\n",
      "    -0.26838228]\n",
      "   [ 0.42330322  0.33436623  0.50310117  0.23135504 -0.197522\n",
      "     0.03115462 -0.5043681  -0.13015777  0.03022042  0.23836412\n",
      "     0.14593932 -0.13206673 -0.42503548 -0.18046175  0.06275025\n",
      "    -0.28125146]\n",
      "   [ 0.42554322  0.2639248   0.5411053   0.13837145 -0.13538112\n",
      "     0.07634766 -0.54822516 -0.10718856 -0.09351212  0.1344352\n",
      "     0.2726983   0.07550688 -0.406095   -0.21499994  0.16692841\n",
      "    -0.29816356]\n",
      "   [ 0.48104748  0.30230832  0.6097446   0.16158499 -0.15650365\n",
      "     0.08383685 -0.6174726  -0.12246653 -0.09891587  0.1577375\n",
      "     0.3013588   0.07393555 -0.46022448 -0.24120131  0.18300715\n",
      "    -0.3362078 ]]]], shape=(1, 2, 6, 16), dtype=float32)\n",
      "Applying SELF ATTENTION:\n",
      "-------------------------------------------------\n",
      "\n",
      "multiplying QUERIE and KEY:\n",
      "tf.Tensor(\n",
      "[[[[-0.02195541 -0.03808806  0.02006753 -0.01217994  0.01551057\n",
      "     0.01601179]\n",
      "   [ 0.00409986  0.0168262  -0.02360738 -0.00307735 -0.02180208\n",
      "    -0.02361995]\n",
      "   [-0.06618665 -0.13468012  0.10109979 -0.02577573  0.08541116\n",
      "     0.09044747]\n",
      "   [-0.03542322 -0.06680368  0.04331908 -0.01670276  0.03544101\n",
      "     0.03719968]\n",
      "   [-0.06659874 -0.13444063  0.09952523 -0.02653014  0.08384486\n",
      "     0.08872118]\n",
      "   [-0.07358623 -0.1482867   0.10943719 -0.02945649  0.09213711\n",
      "     0.09747903]]\n",
      "\n",
      "  [[ 0.04171271  0.00952004  0.09035766  0.05776346  0.09284118\n",
      "     0.10304399]\n",
      "   [ 0.05415793  0.06514544  0.00939617  0.0459158   0.01780668\n",
      "     0.0216838 ]\n",
      "   [-0.00090557 -0.1081269   0.21868375  0.05820424  0.2080269\n",
      "     0.22696212]\n",
      "   [ 0.03317037 -0.02151131  0.1313116   0.06195657  0.13042928\n",
      "     0.14370492]\n",
      "   [ 0.00596353 -0.10137313  0.2229606   0.06485932  0.21322225\n",
      "     0.23291707]\n",
      "   [ 0.0082428  -0.11022273  0.24705456  0.07317783  0.23653138\n",
      "     0.2584469 ]]]], shape=(1, 2, 6, 6), dtype=float32)\n",
      "\n",
      "Taking the dk size:\n",
      "tf.Tensor(16.0, shape=(), dtype=float32)\n",
      "\n",
      "Standarding the process:\n",
      "tf.Tensor(\n",
      "[[[[-0.00548885 -0.00952202  0.00501688 -0.00304498  0.00387764\n",
      "     0.00400295]\n",
      "   [ 0.00102497  0.00420655 -0.00590184 -0.00076934 -0.00545052\n",
      "    -0.00590499]\n",
      "   [-0.01654666 -0.03367003  0.02527495 -0.00644393  0.02135279\n",
      "     0.02261187]\n",
      "   [-0.00885581 -0.01670092  0.01082977 -0.00417569  0.00886025\n",
      "     0.00929992]\n",
      "   [-0.01664969 -0.03361016  0.02488131 -0.00663253  0.02096121\n",
      "     0.02218029]\n",
      "   [-0.01839656 -0.03707168  0.0273593  -0.00736412  0.02303428\n",
      "     0.02436976]]\n",
      "\n",
      "  [[ 0.01042818  0.00238001  0.02258942  0.01444087  0.02321029\n",
      "     0.025761  ]\n",
      "   [ 0.01353948  0.01628636  0.00234904  0.01147895  0.00445167\n",
      "     0.00542095]\n",
      "   [-0.00022639 -0.02703173  0.05467094  0.01455106  0.05200673\n",
      "     0.05674053]\n",
      "   [ 0.00829259 -0.00537783  0.0328279   0.01548914  0.03260732\n",
      "     0.03592623]\n",
      "   [ 0.00149088 -0.02534328  0.05574015  0.01621483  0.05330556\n",
      "     0.05822927]\n",
      "   [ 0.0020607  -0.02755568  0.06176364  0.01829446  0.05913284\n",
      "     0.06461173]]]], shape=(1, 2, 6, 6), dtype=float32)\n",
      "\n",
      "Applying SOFTMAX in the Standarded vlaues, CALCULATING attention weights:\n",
      "tf.Tensor(\n",
      "[[[[0.1658944  0.16522667 0.16764644 0.16630031 0.16745555 0.16747655]\n",
      "   [0.16719247 0.16772525 0.16603836 0.16689274 0.16611332 0.16603784]\n",
      "   [0.1635471  0.16077046 0.17053194 0.16520774 0.1698644  0.17007841]\n",
      "   [0.16520861 0.16391762 0.16849306 0.16598363 0.16816154 0.1682355 ]\n",
      "   [0.16357037 0.16081953 0.17050666 0.16521712 0.16983956 0.17004675]\n",
      "   [0.1632545  0.160234   0.17089789 0.16506557 0.17016034 0.17038774]]\n",
      "\n",
      "  [[0.16565739 0.1643295  0.16768429 0.16632345 0.16778845 0.16821696]\n",
      "   [0.16743597 0.16789654 0.16557275 0.16709132 0.16592124 0.16608214]\n",
      "   [0.1624138  0.15811807 0.17157917 0.16483168 0.17112264 0.17193463]\n",
      "   [0.16471438 0.16247799 0.16880567 0.16590403 0.16876845 0.16932951]\n",
      "   [0.16245204 0.15815075 0.17150839 0.16486168 0.17109133 0.17193581]\n",
      "   [0.16202241 0.15729424 0.17199022 0.1646741  0.17153832 0.17248075]]]], shape=(1, 2, 6, 6), dtype=float32)\n",
      "\n",
      "Calculating the outpu, multipling the attention weights and the VALUE MATRIX:\n",
      "tf.Tensor(\n",
      "[[[[ 0.35581988 -0.21495014 -0.35596293  0.10100652 -0.4739238\n",
      "    -0.4347642  -0.02536879 -0.21185093 -0.08740925 -0.00890876\n",
      "     0.01758493  0.03909552 -0.37537223  0.00078364 -0.33193722\n",
      "    -0.2650398 ]\n",
      "   [ 0.3556555  -0.21588856 -0.35590643  0.10031592 -0.47330797\n",
      "    -0.43401533 -0.02525718 -0.21189839 -0.08842071 -0.01013653\n",
      "     0.01847251  0.03964891 -0.3753579  -0.00054878 -0.3321769\n",
      "    -0.26537973]\n",
      "   [ 0.35611415 -0.21326561 -0.3560635   0.10224567 -0.475028\n",
      "    -0.43610716 -0.02556903 -0.21176532 -0.0855939  -0.00670538\n",
      "     0.01599205  0.03810233 -0.37539715  0.00317479 -0.33150643\n",
      "    -0.2644292 ]\n",
      "   [ 0.35590613 -0.2144565  -0.35599238  0.10136965 -0.4742474\n",
      "    -0.43515772 -0.02542747 -0.21182585 -0.08687727 -0.00826307\n",
      "     0.01711815  0.03880447 -0.3753795   0.00148435 -0.33181098\n",
      "    -0.26486084]\n",
      "   [ 0.35611045 -0.21328293 -0.35606182  0.10223249 -0.47501558\n",
      "    -0.43609235 -0.02556688 -0.21176586 -0.08561281 -0.00672849\n",
      "     0.01600874  0.03811269 -0.37539625  0.00314969 -0.3315104\n",
      "    -0.26443517]\n",
      "   [ 0.35614857 -0.21305852 -0.3560742   0.10239686 -0.47516105\n",
      "    -0.43626958 -0.02559342 -0.2117539  -0.08537137 -0.00643571\n",
      "     0.01579705  0.03798062 -0.37539855  0.00346739 -0.33145225\n",
      "    -0.26435333]]\n",
      "\n",
      "  [[ 0.4071151   0.29958627  0.4946253   0.19381493 -0.17072392\n",
      "     0.04367726 -0.49762434 -0.1179748  -0.00866874  0.19721109\n",
      "     0.17872956 -0.06358342 -0.40232745 -0.18378878  0.09197855\n",
      "    -0.2751933 ]\n",
      "   [ 0.40671754  0.30005595  0.49376923  0.19462007 -0.17122416\n",
      "     0.04315931 -0.49670368 -0.11810932 -0.00735253  0.19812885\n",
      "     0.17722517 -0.06571973 -0.40215826 -0.18325481  0.0907926\n",
      "    -0.27476177]\n",
      "   [ 0.40780085  0.2987358   0.4961217   0.19237347 -0.16982572\n",
      "     0.04459585 -0.4992355  -0.11772955 -0.01100836  0.19556925\n",
      "     0.18139493 -0.05978199 -0.4026075  -0.1847286   0.0940823\n",
      "    -0.27594632]\n",
      "   [ 0.40732098  0.29933846  0.49507093  0.19339195 -0.17046082\n",
      "     0.04394838 -0.49810383 -0.11790363 -0.00935832  0.19672908\n",
      "     0.17951673 -0.06246371 -0.40241373 -0.18406747  0.0925994\n",
      "    -0.27541777]\n",
      "   [ 0.4078002   0.29874662  0.49611542  0.19238791 -0.16983534\n",
      "     0.04458874 -0.49922833 -0.11773307 -0.01098896  0.1955854\n",
      "     0.18137495 -0.05981448 -0.40261018 -0.18472306  0.09406592\n",
      "    -0.27594346]\n",
      "   [ 0.40789875  0.2986358   0.49632487  0.19219567 -0.16971627\n",
      "     0.04471361 -0.4994533  -0.11770157 -0.01130556  0.1953661\n",
      "     0.18173802 -0.05930115 -0.40265375 -0.1848528   0.0943518\n",
      "    -0.27604923]]]], shape=(1, 2, 6, 16), dtype=float32)\n",
      "-------------------------------------------------\n",
      "\n",
      " Transposing the output self attention \n",
      "\n",
      "tf.Tensor(\n",
      "[[[[ 0.35581988 -0.21495014 -0.35596293  0.10100652 -0.4739238\n",
      "    -0.4347642  -0.02536879 -0.21185093 -0.08740925 -0.00890876\n",
      "     0.01758493  0.03909552 -0.37537223  0.00078364 -0.33193722\n",
      "    -0.2650398 ]\n",
      "   [ 0.4071151   0.29958627  0.4946253   0.19381493 -0.17072392\n",
      "     0.04367726 -0.49762434 -0.1179748  -0.00866874  0.19721109\n",
      "     0.17872956 -0.06358342 -0.40232745 -0.18378878  0.09197855\n",
      "    -0.2751933 ]]\n",
      "\n",
      "  [[ 0.3556555  -0.21588856 -0.35590643  0.10031592 -0.47330797\n",
      "    -0.43401533 -0.02525718 -0.21189839 -0.08842071 -0.01013653\n",
      "     0.01847251  0.03964891 -0.3753579  -0.00054878 -0.3321769\n",
      "    -0.26537973]\n",
      "   [ 0.40671754  0.30005595  0.49376923  0.19462007 -0.17122416\n",
      "     0.04315931 -0.49670368 -0.11810932 -0.00735253  0.19812885\n",
      "     0.17722517 -0.06571973 -0.40215826 -0.18325481  0.0907926\n",
      "    -0.27476177]]\n",
      "\n",
      "  [[ 0.35611415 -0.21326561 -0.3560635   0.10224567 -0.475028\n",
      "    -0.43610716 -0.02556903 -0.21176532 -0.0855939  -0.00670538\n",
      "     0.01599205  0.03810233 -0.37539715  0.00317479 -0.33150643\n",
      "    -0.2644292 ]\n",
      "   [ 0.40780085  0.2987358   0.4961217   0.19237347 -0.16982572\n",
      "     0.04459585 -0.4992355  -0.11772955 -0.01100836  0.19556925\n",
      "     0.18139493 -0.05978199 -0.4026075  -0.1847286   0.0940823\n",
      "    -0.27594632]]\n",
      "\n",
      "  [[ 0.35590613 -0.2144565  -0.35599238  0.10136965 -0.4742474\n",
      "    -0.43515772 -0.02542747 -0.21182585 -0.08687727 -0.00826307\n",
      "     0.01711815  0.03880447 -0.3753795   0.00148435 -0.33181098\n",
      "    -0.26486084]\n",
      "   [ 0.40732098  0.29933846  0.49507093  0.19339195 -0.17046082\n",
      "     0.04394838 -0.49810383 -0.11790363 -0.00935832  0.19672908\n",
      "     0.17951673 -0.06246371 -0.40241373 -0.18406747  0.0925994\n",
      "    -0.27541777]]\n",
      "\n",
      "  [[ 0.35611045 -0.21328293 -0.35606182  0.10223249 -0.47501558\n",
      "    -0.43609235 -0.02556688 -0.21176586 -0.08561281 -0.00672849\n",
      "     0.01600874  0.03811269 -0.37539625  0.00314969 -0.3315104\n",
      "    -0.26443517]\n",
      "   [ 0.4078002   0.29874662  0.49611542  0.19238791 -0.16983534\n",
      "     0.04458874 -0.49922833 -0.11773307 -0.01098896  0.1955854\n",
      "     0.18137495 -0.05981448 -0.40261018 -0.18472306  0.09406592\n",
      "    -0.27594346]]\n",
      "\n",
      "  [[ 0.35614857 -0.21305852 -0.3560742   0.10239686 -0.47516105\n",
      "    -0.43626958 -0.02559342 -0.2117539  -0.08537137 -0.00643571\n",
      "     0.01579705  0.03798062 -0.37539855  0.00346739 -0.33145225\n",
      "    -0.26435333]\n",
      "   [ 0.40789875  0.2986358   0.49632487  0.19219567 -0.16971627\n",
      "     0.04471361 -0.4994533  -0.11770157 -0.01130556  0.1953661\n",
      "     0.18173802 -0.05930115 -0.40265375 -0.1848528   0.0943518\n",
      "    -0.27604923]]]], shape=(1, 6, 2, 16), dtype=float32)\n",
      "\n",
      " Concating the Transposed output self attention \n",
      "\n",
      "tf.Tensor(\n",
      "[[[ 0.35581988 -0.21495014 -0.35596293  0.10100652 -0.4739238\n",
      "   -0.4347642  -0.02536879 -0.21185093 -0.08740925 -0.00890876\n",
      "    0.01758493  0.03909552 -0.37537223  0.00078364 -0.33193722\n",
      "   -0.2650398   0.4071151   0.29958627  0.4946253   0.19381493\n",
      "   -0.17072392  0.04367726 -0.49762434 -0.1179748  -0.00866874\n",
      "    0.19721109  0.17872956 -0.06358342 -0.40232745 -0.18378878\n",
      "    0.09197855 -0.2751933 ]\n",
      "  [ 0.3556555  -0.21588856 -0.35590643  0.10031592 -0.47330797\n",
      "   -0.43401533 -0.02525718 -0.21189839 -0.08842071 -0.01013653\n",
      "    0.01847251  0.03964891 -0.3753579  -0.00054878 -0.3321769\n",
      "   -0.26537973  0.40671754  0.30005595  0.49376923  0.19462007\n",
      "   -0.17122416  0.04315931 -0.49670368 -0.11810932 -0.00735253\n",
      "    0.19812885  0.17722517 -0.06571973 -0.40215826 -0.18325481\n",
      "    0.0907926  -0.27476177]\n",
      "  [ 0.35611415 -0.21326561 -0.3560635   0.10224567 -0.475028\n",
      "   -0.43610716 -0.02556903 -0.21176532 -0.0855939  -0.00670538\n",
      "    0.01599205  0.03810233 -0.37539715  0.00317479 -0.33150643\n",
      "   -0.2644292   0.40780085  0.2987358   0.4961217   0.19237347\n",
      "   -0.16982572  0.04459585 -0.4992355  -0.11772955 -0.01100836\n",
      "    0.19556925  0.18139493 -0.05978199 -0.4026075  -0.1847286\n",
      "    0.0940823  -0.27594632]\n",
      "  [ 0.35590613 -0.2144565  -0.35599238  0.10136965 -0.4742474\n",
      "   -0.43515772 -0.02542747 -0.21182585 -0.08687727 -0.00826307\n",
      "    0.01711815  0.03880447 -0.3753795   0.00148435 -0.33181098\n",
      "   -0.26486084  0.40732098  0.29933846  0.49507093  0.19339195\n",
      "   -0.17046082  0.04394838 -0.49810383 -0.11790363 -0.00935832\n",
      "    0.19672908  0.17951673 -0.06246371 -0.40241373 -0.18406747\n",
      "    0.0925994  -0.27541777]\n",
      "  [ 0.35611045 -0.21328293 -0.35606182  0.10223249 -0.47501558\n",
      "   -0.43609235 -0.02556688 -0.21176586 -0.08561281 -0.00672849\n",
      "    0.01600874  0.03811269 -0.37539625  0.00314969 -0.3315104\n",
      "   -0.26443517  0.4078002   0.29874662  0.49611542  0.19238791\n",
      "   -0.16983534  0.04458874 -0.49922833 -0.11773307 -0.01098896\n",
      "    0.1955854   0.18137495 -0.05981448 -0.40261018 -0.18472306\n",
      "    0.09406592 -0.27594346]\n",
      "  [ 0.35614857 -0.21305852 -0.3560742   0.10239686 -0.47516105\n",
      "   -0.43626958 -0.02559342 -0.2117539  -0.08537137 -0.00643571\n",
      "    0.01579705  0.03798062 -0.37539855  0.00346739 -0.33145225\n",
      "   -0.26435333  0.40789875  0.2986358   0.49632487  0.19219567\n",
      "   -0.16971627  0.04471361 -0.4994533  -0.11770157 -0.01130556\n",
      "    0.1953661   0.18173802 -0.05930115 -0.40265375 -0.1848528\n",
      "    0.0943518  -0.27604923]]], shape=(1, 6, 32), dtype=float32)\n",
      "\n",
      " Applying into a dense layer the Concat Transposed output self attention \n",
      "\n",
      "tf.Tensor(\n",
      "[[[-0.13821852 -0.09285098  0.02032005  0.07865915  0.54862016\n",
      "    0.47149926  0.30572945 -0.11539838  0.2509968  -0.3311471\n",
      "    0.12115584 -0.34074268 -0.40778357  0.38316014  0.31482074\n",
      "    0.088976   -0.16216508  0.30293038 -0.00152496  0.46558815\n",
      "    0.46258792 -0.0777663   0.19161822  0.06796408  0.20856254\n",
      "   -0.4714866  -0.16595489  0.4070846   0.21247445  0.23510934\n",
      "   -0.45635027 -0.3265337 ]\n",
      "  [-0.13774487 -0.09235878  0.0187927   0.07670448  0.54750264\n",
      "    0.4708942   0.30620408 -0.11506324  0.24968234 -0.33127466\n",
      "    0.12167352 -0.34085476 -0.40764868  0.38338917  0.3148717\n",
      "    0.0884213  -0.16211866  0.30152407 -0.0018012   0.46590486\n",
      "    0.46237227 -0.07737663  0.19174927  0.06789348  0.20884497\n",
      "   -0.47202185 -0.16438834  0.40590405  0.21133932  0.23495327\n",
      "   -0.45797974 -0.32716542]\n",
      "  [-0.1391684  -0.09385371  0.02305949  0.08217147  0.55055296\n",
      "    0.47243893  0.3046687  -0.11567227  0.25322673 -0.33084968\n",
      "    0.12018611 -0.34048542 -0.4081799   0.38259616  0.31462905\n",
      "    0.08977322 -0.16237606  0.30554274 -0.00104993  0.4648669\n",
      "    0.46278182 -0.07836195  0.19114175  0.0679646   0.20783111\n",
      "   -0.47045764 -0.16875376  0.4093019   0.21446289  0.23507892\n",
      "   -0.45342925 -0.32514206]\n",
      "  [-0.13855559 -0.09317668  0.02123024  0.07968213  0.549215\n",
      "    0.47176257  0.30532575 -0.11548878  0.25162342 -0.33109757\n",
      "    0.12083704 -0.34059066 -0.4079437   0.38289163  0.3146844\n",
      "    0.08917671 -0.16219933  0.30367136 -0.00139859  0.46527976\n",
      "    0.46259305 -0.07801331  0.1914403   0.06797412  0.20827807\n",
      "   -0.47117546 -0.16679266  0.40783808  0.21305864  0.23499833\n",
      "   -0.4553941  -0.32608932]\n",
      "  [-0.13917378 -0.09383928  0.02304033  0.08214556  0.550553\n",
      "    0.4724444   0.30467972 -0.11566981  0.2532101  -0.33084938\n",
      "    0.12018936 -0.3404808  -0.40819955  0.38260627  0.31464055\n",
      "    0.08974823 -0.16236609  0.3055247  -0.0010593   0.46489516\n",
      "    0.4627824  -0.07834893  0.1911501   0.06797311  0.20784016\n",
      "   -0.4704805  -0.16873337  0.40928933  0.21444272  0.23505808\n",
      "   -0.45345467 -0.32512635]\n",
      "  [-0.139269   -0.09391686  0.02337426  0.08263215  0.5508299\n",
      "    0.47269946  0.3045395  -0.11572887  0.25364015 -0.3307254\n",
      "    0.12006947 -0.3404548  -0.40831447  0.3826306   0.31463417\n",
      "    0.08984703 -0.16241968  0.3059745  -0.00100682  0.46484593\n",
      "    0.46290183 -0.07841446  0.19107729  0.06795048  0.2077172\n",
      "   -0.47034386 -0.16922277  0.4094872   0.2146936   0.2351177\n",
      "   -0.45301422 -0.32487506]]], shape=(1, 6, 32), dtype=float32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(TensorShape([1, 6, 32]), TensorShape([1, 2, 6, 6]))"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out, attn = temp_mha(y, k=y, q=y, mask=None)\n",
    "out.shape, attn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InterpretableMultiHeadSelfAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, heads, units, dropout_rate):\n",
    "        super().__init__()\n",
    "        d_att = units // heads\n",
    "        self._queries = tf.keras.layers.Dense(heads*d_att, use_bias=False)\n",
    "        self._keys = tf.keras.layers.Dense(heads*d_att, use_bias=False)\n",
    "        self._values = tf.keras.layers.Dense(d_att, use_bias=False)\n",
    "\n",
    "        self._reshape = tf.keras.layers.TimeDistributed(tf.keras.layers.Reshape([heads, d_att]))\n",
    "        self._softmax = tf.keras.layers.Softmax()\n",
    "        \n",
    "        self._attention = tf.keras.layers.Attention(dropout=dropout_rate)\n",
    "        self._out_weights = tf.keras.layers.Dense(units, use_bias=False)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        query, value, key = inputs, inputs, inputs\n",
    "\n",
    "        query = self._reshape(self._queries(query))\n",
    "        key = self._reshape(self._keys(key))\n",
    "\n",
    "        value = tf.repeat(tf.expand_dims(self._values(value), -2), tf.shape(key)[-2], -2)\n",
    "\n",
    "        outputs, attention = self._attention(\n",
    "            [query/tf.sqrt(tf.cast(tf.shape(key)[-1], float)), value, key],\n",
    "            use_causal_mask=True, return_attention_scores=True\n",
    "        )\n",
    "\n",
    "        outputs = self._out_weights(tf.reduce_mean(outputs, -2))\n",
    "        attention = tf.reduce_mean(attention, -2)\n",
    "\n",
    "        return outputs, attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "teste = InterpretableMultiHeadSelfAttention(2,32,0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-06 09:27:13.189034: W tensorflow/core/framework/op_kernel.cc:1839] OP_REQUIRES failed at transpose_op.cc:142 : INVALID_ARGUMENT: transpose expects a vector of size 4. But input(1) is a vector of size 3\n",
      "2024-06-06 09:27:13.189072: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: INVALID_ARGUMENT: transpose expects a vector of size 4. But input(1) is a vector of size 3\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Exception encountered when calling Attention.call().\n\n\u001b[1m{{function_node __wrapped__Transpose_device_/job:localhost/replica:0/task:0/device:GPU:0}} transpose expects a vector of size 4. But input(1) is a vector of size 3 [Op:Transpose]\u001b[0m\n\nArguments received by Attention.call():\n   inputs=['tf.Tensor(shape=(1, 6, 2, 16), dtype=float32)', 'tf.Tensor(shape=(1, 6, 2, 16), dtype=float32)', 'tf.Tensor(shape=(1, 6, 2, 16), dtype=float32)']\n   mask=['None', 'None', 'None']\n   training=False\n   return_attention_scores=True\n   use_causal_mask=True",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[56], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mteste\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow-park/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "Cell \u001b[0;32mIn[54], line 23\u001b[0m, in \u001b[0;36mInterpretableMultiHeadSelfAttention.call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     19\u001b[0m key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reshape(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_keys(key))\n\u001b[1;32m     21\u001b[0m value \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mrepeat(tf\u001b[38;5;241m.\u001b[39mexpand_dims(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values(value), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m), tf\u001b[38;5;241m.\u001b[39mshape(key)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m], \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m---> 23\u001b[0m outputs, attention \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_attention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43mquery\u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_causal_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_attention_scores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m     26\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_out_weights(tf\u001b[38;5;241m.\u001b[39mreduce_mean(outputs, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m))\n\u001b[1;32m     29\u001b[0m attention \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mreduce_mean(attention, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Exception encountered when calling Attention.call().\n\n\u001b[1m{{function_node __wrapped__Transpose_device_/job:localhost/replica:0/task:0/device:GPU:0}} transpose expects a vector of size 4. But input(1) is a vector of size 3 [Op:Transpose]\u001b[0m\n\nArguments received by Attention.call():\n   inputs=['tf.Tensor(shape=(1, 6, 2, 16), dtype=float32)', 'tf.Tensor(shape=(1, 6, 2, 16), dtype=float32)', 'tf.Tensor(shape=(1, 6, 2, 16), dtype=float32)']\n   mask=['None', 'None', 'None']\n   training=False\n   return_attention_scores=True\n   use_causal_mask=True"
     ]
    }
   ],
   "source": [
    "teste(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow-park",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
