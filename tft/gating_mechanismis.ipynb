{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-11 09:50:48.817833: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-06-11 09:50:48.848910: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-11 09:50:49.310606: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying the temporal fusion transformers gatting mechanismis using tensorflow \n",
    "\n",
    "The teoric text is in my final course work \"\", in 1.6.1 cap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GLU(tf.keras.layers.Layer):\n",
    "    def __init__(self,units):\n",
    "        \"\"\"Define a Gate Linear Unit\n",
    "\n",
    "        Args:\n",
    "            units (int): the units quantities in the dense layer on GLU\n",
    "        \"\"\"\n",
    "        super(GLU,self).__init__()\n",
    "\n",
    "        self.linear = tf.keras.layers.Dense(units)                      #creating a layer to calculate W_{5}x+b_{5}\n",
    "        self.dense_sigmoid = tf.keras.layers.Dense(units,activation='sigmoid') #creating a layer to calculate sigmoid(W_{4}x+b_{4})\n",
    "\n",
    "    def call(self,inputs):\n",
    "        \"\"\"Calculate the element wise multiplication between the layers\n",
    "\n",
    "        Args:\n",
    "            inputs (tf.Tensor,tf.Dataset or np.array): the input to calculate the GLU\n",
    "\n",
    "        Returns:\n",
    "            tf.Tensor,tf.Dataset or np.array: element_wise_product\n",
    "        \"\"\"\n",
    "        element_wise_product = self.dense_sigmoid(inputs) * self.linear(inputs)\n",
    "        return element_wise_product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Drop_GLU_Add_Norm(tf.keras.layers.Layer):\n",
    "    def __init__(self, units, drop_rate):\n",
    "        \"\"\"Define the process of apply a dropout, a GLU add the residuals to GLU output and normalize them\n",
    "\n",
    "        Args:\n",
    "            units (int): the units quantities in the dense layer on GLU\n",
    "            drop_rate (float): a float number 0 <= drop_rate <= 1, where define te dropout rate\n",
    "        \"\"\"\n",
    "        super(Drop_GLU_Add_Norm,self).__init__()\n",
    "        self.units= units\n",
    "        self.drop_rate = drop_rate\n",
    "\n",
    "        self.dropout_layer = tf.keras.layers.Dropout(self.drop_rate)\n",
    "        self.layer_GLU = GLU(self.units)\n",
    "        self.norm_layer = tf.keras.layers.LayerNormalization()\n",
    "\n",
    "    def call(self,inputs, residual):\n",
    "        \"\"\"Compute all process \n",
    "\n",
    "        Args:\n",
    "            inputs (tf.Tensor,tf.Dataset or np.array): real input\n",
    "            residual (tf.Tensor,tf.Dataset or np.array): the residuals input to add to normalize\n",
    "        Returns:\n",
    "            tf.Tensor,tf.Dataset or np.array : normalized_values\n",
    "        \"\"\"\n",
    "        print(\"\\nINPUT\\n\")\n",
    "        print(inputs)\n",
    "        print(\"\\nRESIDUAL\\n\")\n",
    "        print(residual)\n",
    "        input_droped = self.dropout_layer(inputs)\n",
    "        print(\"\\nDROPOUT\\n\")\n",
    "        print(input_droped)\n",
    "        glu_output = self.layer_GLU(input_droped)\n",
    "        print(\"\\GLU\\n\")\n",
    "        print(glu_output)\n",
    "        normalized_values = self.norm_layer(glu_output + residual)\n",
    "        return normalized_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRN(tf.keras.layers.Layer):\n",
    "    def __init__(self, units,d_model,drop_rate, optional_context=False):\n",
    "        super(GRN,self).__init__()\n",
    "\n",
    "        self.units = units\n",
    "        self.d_model = d_model\n",
    "        self.drop_rate = drop_rate\n",
    "        self.optional_context = optional_context\n",
    "\n",
    "        self.layer_ELU = tf.keras.layers.ELU()\n",
    "        self.first_linear = tf.keras.layers.Dense(self.units)\n",
    "        self.second_linear = tf.keras.layers.Dense(self.d_model)\n",
    "\n",
    "        if self.optional_context:\n",
    "            self.linear_optioal = tf.keras.layers.Dense(self.units,use_bias=False)\n",
    "\n",
    "        \n",
    "        self.add_norm = Drop_GLU_Add_Norm(units=self.d_model,\n",
    "                                          drop_rate=self.drop_rate)\n",
    "        \n",
    "    def call(self,inputs):\n",
    "        if self.optional_context:\n",
    "            X, context = inputs\n",
    "            dense_out = self.first_linear(X)\n",
    "            context_out = self.linear_optioal(context)\n",
    "            first_output = self.layer_ELU(dense_out + context_out)\n",
    "        else:\n",
    "            X = inputs\n",
    "            dense_out = self.first_linear(X)\n",
    "            first_output = self.layer_ELU(dense_out)\n",
    "\n",
    "        second_output = self.second_linear(first_output)\n",
    "\n",
    "        final_output = self.add_norm(second_output,X)\n",
    "\n",
    "        return final_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "\n",
    "\n",
    "class VariableSelectionNetwork(tf.keras.layers.Layer):\n",
    "    def __init__(self,units, d_model, dropout_rate, additional_context=None, **kwargs):\n",
    "        super(VariableSelectionNetwork, self).__init__(**kwargs)\n",
    "        self.units = units\n",
    "        self.d_model = d_model\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.additional_context = additional_context\n",
    "\n",
    "        self.flattened_grn = GRN(self.units,d_model= self.d_model,drop_rate=dropout_rate, optional_context=additional_context)\n",
    "        self.per_feature_grn = [GRN(self.units,d_model= self.d_model,drop_rate=dropout_rate) for _ in range(d_model)]\n",
    "\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Non Static Inputs\n",
    "        if self.additional_context:\n",
    "            embedding, static_context = inputs\n",
    "\n",
    "            time_steps = embedding.shape[1]\n",
    "            flatten = tf.reshape(embedding, (-1, time_steps, self.d_model))\n",
    "\n",
    "            static_context = tf.expand_dims(static_context, axis=1)\n",
    "\n",
    "            # Nonlinear transformation with gated residual network.\n",
    "            mlp_outputs = self.flattened_grn((flatten, static_context))\n",
    "\n",
    "            sparse_weights = tf.nn.softmax(mlp_outputs, axis=-1)\n",
    "            sparse_weights = tf.expand_dims(sparse_weights, axis=2)\n",
    "\n",
    "            trans_emb_list = []\n",
    "            for i in range(self.output_size):\n",
    "                e = self.per_feature_grn(embedding[..., i])\n",
    "                trans_emb_list.append(e)\n",
    "            transformed_embedding = tf.concat(trans_emb_list, axis=-1)\n",
    "\n",
    "            combined = sparse_weights * transformed_embedding\n",
    "\n",
    "            temporal_ctx = tf.reduce_sum(combined, axis=-1)\n",
    "\n",
    "        # Static Inputs\n",
    "        else:\n",
    "            embedding = inputs\n",
    "\n",
    "            print(\"\\n my input:\\n\")\n",
    "            print(embedding)\n",
    "            time_steps = embedding.shape[1]\n",
    "            flatten = tf.reshape(embedding, (-1, time_steps, self.d_model))\n",
    "            print(\"\\n my flatten vector:\\n\")\n",
    "            print(flatten)\n",
    "            \n",
    "            mlp_outputs = self.flattened_grn(flatten)\n",
    "            print(\"\\n apply GRN:\\n\")\n",
    "            print(mlp_outputs)\n",
    "            sparse_weights = tf.nn.softmax(mlp_outputs, axis=-1)\n",
    "            print(\"\\n apply the softmax:\\n\")\n",
    "            print(sparse_weights)\n",
    "            sparse_weights = tf.expand_dims(sparse_weights, axis=-1)\n",
    "            print(\"\\n expanding the dimensions:\\n\")\n",
    "            print(sparse_weights)\n",
    "\n",
    "            trans_emb_list = []\n",
    "            for i in range(time_steps):\n",
    "                #e = self.per_feature_grn(tf.expand_dims(embedding[: , :, i], axis=1))\n",
    "                e = self.per_feature_grn[i](tf.expand_dims(embedding[:,:,i],axis=1))\n",
    "                trans_emb_list.append(e)\n",
    "            transformed_embedding = tf.concat(trans_emb_list, axis=1)\n",
    "            print(f\"\\n concat the elements:\\n\")\n",
    "            print(transformed_embedding)\n",
    "\n",
    "            combined = sparse_weights * transformed_embedding\n",
    "            print(f\"\\n multiplying the weights and transformed elements :\\n\")\n",
    "            print(transformed_embedding)\n",
    "            temporal_ctx = tf.reduce_sum(combined, axis=1)\n",
    "            print(f\"\\n sum all:\\n\")\n",
    "            print(temporal_ctx)\n",
    "        return temporal_ctx, sparse_weights\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = tf.random.uniform(shape=(1, 8, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 8, 6), dtype=float32, numpy=\n",
       "array([[[0.5616244 , 0.28085256, 0.1318121 , 0.6902391 , 0.20553458,\n",
       "         0.5646901 ],\n",
       "        [0.7794831 , 0.98759806, 0.14046597, 0.8220762 , 0.24682856,\n",
       "         0.23075736],\n",
       "        [0.6895392 , 0.43073773, 0.91565204, 0.8308842 , 0.47240186,\n",
       "         0.25788152],\n",
       "        [0.09519708, 0.411788  , 0.07336771, 0.7346499 , 0.33646667,\n",
       "         0.29076874],\n",
       "        [0.17374647, 0.42647767, 0.36155653, 0.19456589, 0.79776347,\n",
       "         0.42057192],\n",
       "        [0.38651323, 0.12302053, 0.44405568, 0.5620618 , 0.93665445,\n",
       "         0.93753755],\n",
       "        [0.5958245 , 0.8004904 , 0.94326925, 0.5396719 , 0.9968792 ,\n",
       "         0.18966377],\n",
       "        [0.47886646, 0.5347481 , 0.90754366, 0.7479423 , 0.33037877,\n",
       "         0.27249277]]], dtype=float32)>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " my input:\n",
      "\n",
      "tf.Tensor(\n",
      "[[[0.5616244  0.28085256 0.1318121  0.6902391  0.20553458 0.5646901 ]\n",
      "  [0.7794831  0.98759806 0.14046597 0.8220762  0.24682856 0.23075736]\n",
      "  [0.6895392  0.43073773 0.91565204 0.8308842  0.47240186 0.25788152]\n",
      "  [0.09519708 0.411788   0.07336771 0.7346499  0.33646667 0.29076874]\n",
      "  [0.17374647 0.42647767 0.36155653 0.19456589 0.79776347 0.42057192]\n",
      "  [0.38651323 0.12302053 0.44405568 0.5620618  0.93665445 0.93753755]\n",
      "  [0.5958245  0.8004904  0.94326925 0.5396719  0.9968792  0.18966377]\n",
      "  [0.47886646 0.5347481  0.90754366 0.7479423  0.33037877 0.27249277]]], shape=(1, 8, 6), dtype=float32)\n",
      "\n",
      " my flatten vector:\n",
      "\n",
      "Tensor(\"Reshape:0\", shape=(1, 8, 6), dtype=float32)\n",
      "\n",
      "INPUT\n",
      "\n",
      "Tensor(\"dense_343_1/Add:0\", shape=(1, 8, 6), dtype=float32)\n",
      "\n",
      "RESIDUAL\n",
      "\n",
      "Tensor(\"Reshape:0\", shape=(1, 8, 6), dtype=float32)\n",
      "\n",
      "DROPOUT\n",
      "\n",
      "Tensor(\"dense_343_1/Add:0\", shape=(1, 8, 6), dtype=float32)\n",
      "\\GLU\n",
      "\n",
      "Tensor(\"glu_86_1/mul:0\", shape=(1, 8, 6), dtype=float32)\n",
      "\n",
      "INPUT\n",
      "\n",
      "Tensor(\"dense_343_1/Add:0\", shape=(1, 8, 6), dtype=float32)\n",
      "\n",
      "RESIDUAL\n",
      "\n",
      "Tensor(\"Reshape:0\", shape=(1, 8, 6), dtype=float32)\n",
      "\n",
      "DROPOUT\n",
      "\n",
      "Tensor(\"dense_343_1/Add:0\", shape=(1, 8, 6), dtype=float32)\n",
      "\\GLU\n",
      "\n",
      "Tensor(\"drop_glu__add__norm_86_1/glu_86_1/mul:0\", shape=(1, 8, 6), dtype=float32)\n",
      "\n",
      "INPUT\n",
      "\n",
      "Tensor(\"grn_85_1/dense_343_1/Add:0\", shape=(1, 8, 6), dtype=float32)\n",
      "\n",
      "RESIDUAL\n",
      "\n",
      "Tensor(\"Reshape:0\", shape=(1, 8, 6), dtype=float32)\n",
      "\n",
      "DROPOUT\n",
      "\n",
      "Tensor(\"grn_85_1/dense_343_1/Add:0\", shape=(1, 8, 6), dtype=float32)\n",
      "\\GLU\n",
      "\n",
      "Tensor(\"grn_85_1/drop_glu__add__norm_86_1/glu_86_1/mul:0\", shape=(1, 8, 6), dtype=float32)\n",
      "\n",
      " apply GRN:\n",
      "\n",
      "Tensor(\"grn_85_1/drop_glu__add__norm_86_1/layer_normalization_86_1/add_2:0\", shape=(1, 8, 6), dtype=float32)\n",
      "\n",
      " apply the softmax:\n",
      "\n",
      "Tensor(\"Softmax:0\", shape=(1, 8, 6), dtype=float32)\n",
      "\n",
      " expanding the dimensions:\n",
      "\n",
      "Tensor(\"ExpandDims:0\", shape=(1, 8, 6, 1), dtype=float32)\n",
      "\n",
      "INPUT\n",
      "\n",
      "Tensor(\"dense_347_1/Add:0\", shape=(1, 1, 6), dtype=float32)\n",
      "\n",
      "RESIDUAL\n",
      "\n",
      "Tensor(\"ExpandDims_1:0\", shape=(1, 1, 8), dtype=float32)\n",
      "\n",
      "DROPOUT\n",
      "\n",
      "Tensor(\"dense_347_1/Add:0\", shape=(1, 1, 6), dtype=float32)\n",
      "\\GLU\n",
      "\n",
      "Tensor(\"glu_87_1/mul:0\", shape=(1, 1, 6), dtype=float32)\n",
      "\n",
      "INPUT\n",
      "\n",
      "Tensor(\"dense_347_1/Add:0\", shape=(1, 1, 6), dtype=float32)\n",
      "\n",
      "RESIDUAL\n",
      "\n",
      "Tensor(\"ExpandDims_1:0\", shape=(1, 1, 8), dtype=float32)\n",
      "\n",
      "DROPOUT\n",
      "\n",
      "Tensor(\"dense_347_1/Add:0\", shape=(1, 1, 6), dtype=float32)\n",
      "\\GLU\n",
      "\n",
      "Tensor(\"drop_glu__add__norm_87_1/glu_87_1/mul:0\", shape=(1, 1, 6), dtype=float32)\n",
      "\n",
      "INPUT\n",
      "\n",
      "Tensor(\"grn_86_1/dense_347_1/Add:0\", shape=(1, 1, 6), dtype=float32)\n",
      "\n",
      "RESIDUAL\n",
      "\n",
      "Tensor(\"ExpandDims_1:0\", shape=(1, 1, 8), dtype=float32)\n",
      "\n",
      "DROPOUT\n",
      "\n",
      "Tensor(\"grn_86_1/dense_347_1/Add:0\", shape=(1, 1, 6), dtype=float32)\n",
      "\\GLU\n",
      "\n",
      "Tensor(\"glu_87_1/mul:0\", shape=(1, 1, 6), dtype=float32)\n",
      "\n",
      "INPUT\n",
      "\n",
      "Tensor(\"grn_86_1/dense_347_1/Add:0\", shape=(1, 1, 6), dtype=float32)\n",
      "\n",
      "RESIDUAL\n",
      "\n",
      "Tensor(\"ExpandDims_1:0\", shape=(1, 1, 8), dtype=float32)\n",
      "\n",
      "DROPOUT\n",
      "\n",
      "Tensor(\"grn_86_1/dense_347_1/Add:0\", shape=(1, 1, 6), dtype=float32)\n",
      "\\GLU\n",
      "\n",
      "Tensor(\"grn_86_1/drop_glu__add__norm_87_1/glu_87_1/mul:0\", shape=(1, 1, 6), dtype=float32)\n",
      "\n",
      " my input:\n",
      "\n",
      "tf.Tensor(\n",
      "[[[0.5616244  0.28085256 0.1318121  0.6902391  0.20553458 0.5646901 ]\n",
      "  [0.7794831  0.98759806 0.14046597 0.8220762  0.24682856 0.23075736]\n",
      "  [0.6895392  0.43073773 0.91565204 0.8308842  0.47240186 0.25788152]\n",
      "  [0.09519708 0.411788   0.07336771 0.7346499  0.33646667 0.29076874]\n",
      "  [0.17374647 0.42647767 0.36155653 0.19456589 0.79776347 0.42057192]\n",
      "  [0.38651323 0.12302053 0.44405568 0.5620618  0.93665445 0.93753755]\n",
      "  [0.5958245  0.8004904  0.94326925 0.5396719  0.9968792  0.18966377]\n",
      "  [0.47886646 0.5347481  0.90754366 0.7479423  0.33037877 0.27249277]]], shape=(1, 8, 6), dtype=float32)\n",
      "\n",
      " my flatten vector:\n",
      "\n",
      "tf.Tensor(\n",
      "[[[0.5616244  0.28085256 0.1318121  0.6902391  0.20553458 0.5646901 ]\n",
      "  [0.7794831  0.98759806 0.14046597 0.8220762  0.24682856 0.23075736]\n",
      "  [0.6895392  0.43073773 0.91565204 0.8308842  0.47240186 0.25788152]\n",
      "  [0.09519708 0.411788   0.07336771 0.7346499  0.33646667 0.29076874]\n",
      "  [0.17374647 0.42647767 0.36155653 0.19456589 0.79776347 0.42057192]\n",
      "  [0.38651323 0.12302053 0.44405568 0.5620618  0.93665445 0.93753755]\n",
      "  [0.5958245  0.8004904  0.94326925 0.5396719  0.9968792  0.18966377]\n",
      "  [0.47886646 0.5347481  0.90754366 0.7479423  0.33037877 0.27249277]]], shape=(1, 8, 6), dtype=float32)\n",
      "\n",
      "INPUT\n",
      "\n",
      "tf.Tensor(\n",
      "[[[-0.03371662  0.24083424 -0.3705      0.24315827 -0.12671982\n",
      "   -0.38682663]\n",
      "  [-0.00256222  0.06374684 -0.1546434   0.61100805 -0.08892305\n",
      "   -0.5591236 ]\n",
      "  [ 0.31571698  0.37765118 -0.17109454  0.18385506  0.09269914\n",
      "   -0.35352737]\n",
      "  [ 0.09398024  0.3067182  -0.1915065  -0.00106094 -0.04886502\n",
      "   -0.2351719 ]\n",
      "  [ 0.23389125  0.3798894  -0.03931243  0.02894221 -0.24411726\n",
      "   -0.26120958]\n",
      "  [ 0.2022171   0.6797363  -0.33641914 -0.07353216 -0.350346\n",
      "   -0.33181864]\n",
      "  [ 0.4748701   0.4385088   0.07197549  0.27391985 -0.06208608\n",
      "   -0.40440506]\n",
      "  [ 0.23916647  0.26005456 -0.18961751  0.05736905  0.06983872\n",
      "   -0.33738446]]], shape=(1, 8, 6), dtype=float32)\n",
      "\n",
      "RESIDUAL\n",
      "\n",
      "tf.Tensor(\n",
      "[[[0.5616244  0.28085256 0.1318121  0.6902391  0.20553458 0.5646901 ]\n",
      "  [0.7794831  0.98759806 0.14046597 0.8220762  0.24682856 0.23075736]\n",
      "  [0.6895392  0.43073773 0.91565204 0.8308842  0.47240186 0.25788152]\n",
      "  [0.09519708 0.411788   0.07336771 0.7346499  0.33646667 0.29076874]\n",
      "  [0.17374647 0.42647767 0.36155653 0.19456589 0.79776347 0.42057192]\n",
      "  [0.38651323 0.12302053 0.44405568 0.5620618  0.93665445 0.93753755]\n",
      "  [0.5958245  0.8004904  0.94326925 0.5396719  0.9968792  0.18966377]\n",
      "  [0.47886646 0.5347481  0.90754366 0.7479423  0.33037877 0.27249277]]], shape=(1, 8, 6), dtype=float32)\n",
      "\n",
      "DROPOUT\n",
      "\n",
      "tf.Tensor(\n",
      "[[[-0.03371662  0.24083424 -0.3705      0.24315827 -0.12671982\n",
      "   -0.38682663]\n",
      "  [-0.00256222  0.06374684 -0.1546434   0.61100805 -0.08892305\n",
      "   -0.5591236 ]\n",
      "  [ 0.31571698  0.37765118 -0.17109454  0.18385506  0.09269914\n",
      "   -0.35352737]\n",
      "  [ 0.09398024  0.3067182  -0.1915065  -0.00106094 -0.04886502\n",
      "   -0.2351719 ]\n",
      "  [ 0.23389125  0.3798894  -0.03931243  0.02894221 -0.24411726\n",
      "   -0.26120958]\n",
      "  [ 0.2022171   0.6797363  -0.33641914 -0.07353216 -0.350346\n",
      "   -0.33181864]\n",
      "  [ 0.4748701   0.4385088   0.07197549  0.27391985 -0.06208608\n",
      "   -0.40440506]\n",
      "  [ 0.23916647  0.26005456 -0.18961751  0.05736905  0.06983872\n",
      "   -0.33738446]]], shape=(1, 8, 6), dtype=float32)\n",
      "\\GLU\n",
      "\n",
      "tf.Tensor(\n",
      "[[[-0.09758551  0.1174698   0.25222182 -0.13024431  0.08740004\n",
      "    0.05288811]\n",
      "  [-0.01829934  0.01346427  0.42884997 -0.07015774  0.03767585\n",
      "    0.16987062]\n",
      "  [ 0.05416359 -0.04012852  0.25822437 -0.25255337  0.08889496\n",
      "    0.26459345]\n",
      "  [-0.04876538  0.03363642  0.09494188 -0.18148196  0.06113771\n",
      "    0.09538933]\n",
      "  [-0.02653998  0.02194739  0.02563956 -0.20678513 -0.0248195\n",
      "    0.22377247]\n",
      "  [-0.14087313  0.19385614  0.0279766  -0.3596769   0.06408049\n",
      "    0.23142794]\n",
      "  [ 0.0953626  -0.1177027   0.18288444 -0.25864166  0.00686897\n",
      "    0.4552924 ]\n",
      "  [ 0.02921579 -0.05227069  0.21056916 -0.21928634  0.0604657\n",
      "    0.14134443]]], shape=(1, 8, 6), dtype=float32)\n",
      "\n",
      " apply GRN:\n",
      "\n",
      "tf.Tensor(\n",
      "[[[ 0.09840751 -0.47788215 -0.60318184  0.9398749  -1.4020607\n",
      "    1.444843  ]\n",
      "  [ 0.54991126  1.541122   -0.24291277  0.51162577 -1.4197927\n",
      "   -0.9399543 ]\n",
      "  [ 0.3236823  -1.0703117   2.021984   -0.3291967  -0.39644623\n",
      "   -0.54971266]\n",
      "  [-1.638426    0.6439754  -0.9412346   1.2603122   0.3704251\n",
      "    0.30494773]\n",
      "  [-0.9248922   0.18603683 -0.03978264 -1.5128728   1.3829002\n",
      "    0.9086101 ]\n",
      "  [-0.84875476 -0.6610606  -0.25225627 -0.9627237   1.1407706\n",
      "    1.5840245 ]\n",
      "  [-0.17207003 -0.20274019  1.416198   -1.6697456   0.9692378\n",
      "   -0.34088063]\n",
      "  [-0.26198387 -0.36426234  2.174782   -0.17980194 -0.73028994\n",
      "   -0.6384455 ]]], shape=(1, 8, 6), dtype=float32)\n",
      "\n",
      " apply the softmax:\n",
      "\n",
      "tf.Tensor(\n",
      "[[[0.11842349 0.06655159 0.05871397 0.27471507 0.02641148 0.4551844 ]\n",
      "  [0.18266846 0.49219918 0.08266934 0.1758071  0.02548191 0.04117401]\n",
      "  [0.12288774 0.0304863  0.67153966 0.0639686  0.0598082  0.05130952]\n",
      "  [0.02202811 0.21587962 0.04423477 0.3998373  0.16421402 0.15380618]\n",
      "  [0.04287458 0.13021776 0.10389583 0.02381459 0.43098426 0.268213  ]\n",
      "  [0.04234228 0.05108444 0.07688297 0.03778141 0.30960935 0.48229948]\n",
      "  [0.09038129 0.08765136 0.4424402  0.02021372 0.2829712  0.07634226]\n",
      "  [0.06354542 0.05736742 0.7267052  0.06898829 0.03978337 0.04361029]]], shape=(1, 8, 6), dtype=float32)\n",
      "\n",
      " expanding the dimensions:\n",
      "\n",
      "tf.Tensor(\n",
      "[[[[0.11842349]\n",
      "   [0.06655159]\n",
      "   [0.05871397]\n",
      "   [0.27471507]\n",
      "   [0.02641148]\n",
      "   [0.4551844 ]]\n",
      "\n",
      "  [[0.18266846]\n",
      "   [0.49219918]\n",
      "   [0.08266934]\n",
      "   [0.1758071 ]\n",
      "   [0.02548191]\n",
      "   [0.04117401]]\n",
      "\n",
      "  [[0.12288774]\n",
      "   [0.0304863 ]\n",
      "   [0.67153966]\n",
      "   [0.0639686 ]\n",
      "   [0.0598082 ]\n",
      "   [0.05130952]]\n",
      "\n",
      "  [[0.02202811]\n",
      "   [0.21587962]\n",
      "   [0.04423477]\n",
      "   [0.3998373 ]\n",
      "   [0.16421402]\n",
      "   [0.15380618]]\n",
      "\n",
      "  [[0.04287458]\n",
      "   [0.13021776]\n",
      "   [0.10389583]\n",
      "   [0.02381459]\n",
      "   [0.43098426]\n",
      "   [0.268213  ]]\n",
      "\n",
      "  [[0.04234228]\n",
      "   [0.05108444]\n",
      "   [0.07688297]\n",
      "   [0.03778141]\n",
      "   [0.30960935]\n",
      "   [0.48229948]]\n",
      "\n",
      "  [[0.09038129]\n",
      "   [0.08765136]\n",
      "   [0.4424402 ]\n",
      "   [0.02021372]\n",
      "   [0.2829712 ]\n",
      "   [0.07634226]]\n",
      "\n",
      "  [[0.06354542]\n",
      "   [0.05736742]\n",
      "   [0.7267052 ]\n",
      "   [0.06898829]\n",
      "   [0.03978337]\n",
      "   [0.04361029]]]], shape=(1, 8, 6, 1), dtype=float32)\n",
      "\n",
      "INPUT\n",
      "\n",
      "Tensor(\"dense_347_1/Add:0\", shape=(1, 1, 6), dtype=float32)\n",
      "\n",
      "RESIDUAL\n",
      "\n",
      "tf.Tensor(\n",
      "[[[0.5616244  0.7794831  0.6895392  0.09519708 0.17374647 0.38651323\n",
      "   0.5958245  0.47886646]]], shape=(1, 1, 8), dtype=float32)\n",
      "\n",
      "DROPOUT\n",
      "\n",
      "Tensor(\"dense_347_1/Add:0\", shape=(1, 1, 6), dtype=float32)\n",
      "\\GLU\n",
      "\n",
      "Tensor(\"glu_87_1/mul:0\", shape=(1, 1, 6), dtype=float32)\n",
      "\n",
      "INPUT\n",
      "\n",
      "Tensor(\"dense_347_1/Add:0\", shape=(1, 1, 6), dtype=float32)\n",
      "\n",
      "RESIDUAL\n",
      "\n",
      "tf.Tensor(\n",
      "[[[0.5616244  0.7794831  0.6895392  0.09519708 0.17374647 0.38651323\n",
      "   0.5958245  0.47886646]]], shape=(1, 1, 8), dtype=float32)\n",
      "\n",
      "DROPOUT\n",
      "\n",
      "Tensor(\"dense_347_1/Add:0\", shape=(1, 1, 6), dtype=float32)\n",
      "\\GLU\n",
      "\n",
      "Tensor(\"drop_glu__add__norm_87_1/glu_87_1/mul:0\", shape=(1, 1, 6), dtype=float32)\n",
      "\n",
      "INPUT\n",
      "\n",
      "tf.Tensor(\n",
      "[[[-0.1754365  -0.37091678  0.5597345  -0.58487314  0.50785494\n",
      "    1.0420551 ]]], shape=(1, 1, 6), dtype=float32)\n",
      "\n",
      "RESIDUAL\n",
      "\n",
      "tf.Tensor(\n",
      "[[[0.5616244  0.7794831  0.6895392  0.09519708 0.17374647 0.38651323\n",
      "   0.5958245  0.47886646]]], shape=(1, 1, 8), dtype=float32)\n",
      "\n",
      "DROPOUT\n",
      "\n",
      "tf.Tensor(\n",
      "[[[-0.1754365  -0.37091678  0.5597345  -0.58487314  0.50785494\n",
      "    1.0420551 ]]], shape=(1, 1, 6), dtype=float32)\n",
      "\\GLU\n",
      "\n",
      "Tensor(\"glu_87_1/mul:0\", shape=(1, 1, 6), dtype=float32)\n",
      "\n",
      "INPUT\n",
      "\n",
      "tf.Tensor(\n",
      "[[[-0.1754365  -0.37091678  0.5597345  -0.58487314  0.50785494\n",
      "    1.0420551 ]]], shape=(1, 1, 6), dtype=float32)\n",
      "\n",
      "RESIDUAL\n",
      "\n",
      "tf.Tensor(\n",
      "[[[0.5616244  0.7794831  0.6895392  0.09519708 0.17374647 0.38651323\n",
      "   0.5958245  0.47886646]]], shape=(1, 1, 8), dtype=float32)\n",
      "\n",
      "DROPOUT\n",
      "\n",
      "tf.Tensor(\n",
      "[[[-0.1754365  -0.37091678  0.5597345  -0.58487314  0.50785494\n",
      "    1.0420551 ]]], shape=(1, 1, 6), dtype=float32)\n",
      "\\GLU\n",
      "\n",
      "tf.Tensor(\n",
      "[[[-0.15979896 -0.33342075 -0.02294477 -0.07853901  0.6734325\n",
      "    0.02530193]]], shape=(1, 1, 6), dtype=float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-11 10:17:38.181428: W tensorflow/core/framework/op_kernel.cc:1827] INVALID_ARGUMENT: required broadcastable shapes\n",
      "2024-06-11 10:17:38.181451: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: INVALID_ARGUMENT: required broadcastable shapes\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Exception encountered when calling Drop_GLU_Add_Norm.call().\n\n\u001b[1m{{function_node __wrapped__AddV2_device_/job:localhost/replica:0/task:0/device:GPU:0}} required broadcastable shapes [Op:AddV2] name: \u001b[0m\n\nArguments received by Drop_GLU_Add_Norm.call():\n  • inputs=tf.Tensor(shape=(1, 1, 6), dtype=float32)\n  • residual=tf.Tensor(shape=(1, 1, 8), dtype=float32)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[64], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m VariableSelectionNetwork(units\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m,d_model\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m6\u001b[39m,dropout_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow-park/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "Cell \u001b[0;32mIn[61], line 68\u001b[0m, in \u001b[0;36mVariableSelectionNetwork.call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     65\u001b[0m trans_emb_list \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(time_steps):\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;66;03m#e = self.per_feature_grn(tf.expand_dims(embedding[: , :, i], axis=1))\u001b[39;00m\n\u001b[0;32m---> 68\u001b[0m     e \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mper_feature_grn\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexpand_dims\u001b[49m\u001b[43m(\u001b[49m\u001b[43membedding\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     69\u001b[0m     trans_emb_list\u001b[38;5;241m.\u001b[39mappend(e)\n\u001b[1;32m     70\u001b[0m transformed_embedding \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mconcat(trans_emb_list, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "Cell \u001b[0;32mIn[44], line 34\u001b[0m, in \u001b[0;36mGRN.call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     30\u001b[0m     first_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_ELU(dense_out)\n\u001b[1;32m     32\u001b[0m second_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msecond_linear(first_output)\n\u001b[0;32m---> 34\u001b[0m final_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_norm\u001b[49m\u001b[43m(\u001b[49m\u001b[43msecond_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m final_output\n",
      "Cell \u001b[0;32mIn[38], line 36\u001b[0m, in \u001b[0;36mDrop_GLU_Add_Norm.call\u001b[0;34m(self, inputs, residual)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mGLU\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28mprint\u001b[39m(glu_output)\n\u001b[0;32m---> 36\u001b[0m normalized_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm_layer(\u001b[43mglu_output\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mresidual\u001b[49m)\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m normalized_values\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Exception encountered when calling Drop_GLU_Add_Norm.call().\n\n\u001b[1m{{function_node __wrapped__AddV2_device_/job:localhost/replica:0/task:0/device:GPU:0}} required broadcastable shapes [Op:AddV2] name: \u001b[0m\n\nArguments received by Drop_GLU_Add_Norm.call():\n  • inputs=tf.Tensor(shape=(1, 1, 6), dtype=float32)\n  • residual=tf.Tensor(shape=(1, 1, 8), dtype=float32)"
     ]
    }
   ],
   "source": [
    "model = VariableSelectionNetwork(units=32,d_model=6,dropout_rate=0.0)\n",
    "model(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "per_feature_grn = [GRN(32,d_model= 6,drop_rate=0.0) for _ in range(6)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "per_feature_grn = [GRN(32,d_model= 6,drop_rate=0.0) for _ in range(6)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-11 10:01:44.648334: W tensorflow/core/framework/op_kernel.cc:1827] INVALID_ARGUMENT: required broadcastable shapes\n",
      "2024-06-11 10:01:44.648360: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: INVALID_ARGUMENT: required broadcastable shapes\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Exception encountered when calling Drop_GLU_Add_Norm.call().\n\n\u001b[1m{{function_node __wrapped__AddV2_device_/job:localhost/replica:0/task:0/device:GPU:0}} required broadcastable shapes [Op:AddV2] name: \u001b[0m\n\nArguments received by Drop_GLU_Add_Norm.call():\n  • inputs=tf.Tensor(shape=(1, 1, 32), dtype=float32)\n  • residual=tf.Tensor(shape=(1, 1, 8), dtype=float32)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m teste_GRN \u001b[38;5;241m=\u001b[39m GRN(units\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m,d_model\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m6\u001b[39m,drop_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m6\u001b[39m)):\n\u001b[0;32m----> 5\u001b[0m     e \u001b[38;5;241m=\u001b[39m \u001b[43mper_feature_grn\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexpand_dims\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m element \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28mprint\u001b[39m(e)\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow-park/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "Cell \u001b[0;32mIn[4], line 34\u001b[0m, in \u001b[0;36mGRN.call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     30\u001b[0m     first_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_ELU(dense_out)\n\u001b[1;32m     32\u001b[0m second_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msecond_linear(first_output)\n\u001b[0;32m---> 34\u001b[0m final_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_norm\u001b[49m\u001b[43m(\u001b[49m\u001b[43msecond_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m final_output\n",
      "Cell \u001b[0;32mIn[3], line 28\u001b[0m, in \u001b[0;36mDrop_GLU_Add_Norm.call\u001b[0;34m(self, inputs, residual)\u001b[0m\n\u001b[1;32m     26\u001b[0m input_droped \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout_layer(inputs)\n\u001b[1;32m     27\u001b[0m glu_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_GLU(input_droped)\n\u001b[0;32m---> 28\u001b[0m normalized_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm_layer(\u001b[43mglu_output\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mresidual\u001b[49m)\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m normalized_values\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Exception encountered when calling Drop_GLU_Add_Norm.call().\n\n\u001b[1m{{function_node __wrapped__AddV2_device_/job:localhost/replica:0/task:0/device:GPU:0}} required broadcastable shapes [Op:AddV2] name: \u001b[0m\n\nArguments received by Drop_GLU_Add_Norm.call():\n  • inputs=tf.Tensor(shape=(1, 1, 32), dtype=float32)\n  • residual=tf.Tensor(shape=(1, 1, 8), dtype=float32)"
     ]
    }
   ],
   "source": [
    "testando = tf.random.uniform(shape=(1, 8, 6, 1))\n",
    "\n",
    "teste_GRN = GRN(units=32,d_model=6,drop_rate=0.0)\n",
    "for i in list(range(6)):\n",
    "    e = per_feature_grn[i](tf.expand_dims(y[: , :, i], axis=1))\n",
    "    print(f\"\\n element {i}:\")\n",
    "    print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'GRN' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[130], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m6\u001b[39m)):\n\u001b[0;32m----> 2\u001b[0m     e \u001b[38;5;241m=\u001b[39m per_feature_grn[i](\u001b[43mteste_GRN\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;66;03m#print(f\"\\n element {i}:\")\u001b[39;00m\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;66;03m#print(e)\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;66;03m#trans_emb_list.append(e)\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'GRN' object is not subscriptable"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 8, 32), dtype=float32, numpy=\n",
       "array([[[ 0.6773038 , -0.38210535, -0.4436226 ,  0.46208906,\n",
       "         -0.32801628, -0.597528  ,  1.055737  , -0.9961195 ,\n",
       "         -0.3975935 ,  0.93579245,  1.456058  , -0.06971025,\n",
       "          0.76543474, -0.5420809 ,  1.6719556 ,  0.45393038,\n",
       "         -0.84841824,  0.09780025,  1.0665584 ,  1.0427299 ,\n",
       "          0.19485426, -0.6762171 ,  0.736012  ,  1.4454112 ,\n",
       "         -0.7830076 , -1.4795971 , -0.29632902, -1.7841425 ,\n",
       "         -1.6429558 , -1.752029  ,  0.50096846,  0.45682096],\n",
       "        [ 0.66930056, -0.37932014, -0.44489098,  0.45336866,\n",
       "         -0.3245635 , -0.5956259 ,  1.0321364 , -0.98268366,\n",
       "         -0.39986134,  0.9176259 ,  1.4498649 , -0.07090378,\n",
       "          0.769279  , -0.538311  ,  1.6430497 ,  0.45401335,\n",
       "         -0.83765316,  0.09722519,  1.0759196 ,  1.0496573 ,\n",
       "          0.19520426, -0.67557526,  0.71922636,  1.4293017 ,\n",
       "         -0.78772306, -1.4676156 , -0.2966051 , -1.7494726 ,\n",
       "         -1.6333203 , -1.7166891 ,  0.4904027 ,  0.45522165],\n",
       "        [ 0.63758373, -0.36542034, -0.43902206,  0.42428303,\n",
       "         -0.31019783, -0.5794654 ,  0.957222  , -0.93229914,\n",
       "         -0.39720058,  0.8575773 ,  1.4068289 , -0.0722723 ,\n",
       "          0.76294994, -0.51899433,  1.5439081 ,  0.445199  ,\n",
       "         -0.79629374,  0.09395552,  1.0763807 ,  1.0449467 ,\n",
       "          0.19214344, -0.6607299 ,  0.66624546,  1.3638659 ,\n",
       "         -0.7830558 , -1.4109769 , -0.29138708, -1.6347718 ,\n",
       "         -1.578785  , -1.6010308 ,  0.45632935,  0.44249153],\n",
       "        [ 0.6683879 , -0.3789749 , -0.4449315 ,  0.45242643,\n",
       "         -0.32416344, -0.5953274 ,  1.0296226 , -0.9811783 ,\n",
       "         -0.40000916,  0.91566896,  1.4489837 , -0.07100725,\n",
       "          0.76951265, -0.5378394 ,  1.6399007 ,  0.45393705,\n",
       "         -0.8364372 ,  0.0971508 ,  1.0766416 ,  1.0501437 ,\n",
       "          0.19520283, -0.675385  ,  0.7174411 ,  1.4274497 ,\n",
       "         -0.7880392 , -1.4661622 , -0.2965765 , -1.7457328 ,\n",
       "         -1.6320662 , -1.7128882 ,  0.4892726 ,  0.45497942],\n",
       "        [ 0.12395084, -0.07320833, -0.09333551,  0.07850933,\n",
       "         -0.06085861, -0.11899328,  0.17243767, -0.17930567,\n",
       "         -0.08577549,  0.15806246,  0.28691363, -0.01654613,\n",
       "          0.16413832, -0.10417557,  0.2890588 ,  0.09318876,\n",
       "         -0.15398765,  0.01895177,  0.23635626,  0.22681773,\n",
       "          0.04059315, -0.1374582 ,  0.11960769,  0.26637447,\n",
       "         -0.16939044, -0.28112257, -0.06129479, -0.30108714,\n",
       "         -0.31893873, -0.29315925,  0.08304429,  0.09063184],\n",
       "        [ 0.6277299 , -0.36059284, -0.43528128,  0.4162073 ,\n",
       "         -0.30561066, -0.5729246 ,  0.9372244 , -0.91713715,\n",
       "         -0.39432144,  0.8410139 ,  1.3902092 , -0.07210445,\n",
       "          0.757184  , -0.51221704,  1.5158014 ,  0.4408536 ,\n",
       "         -0.7836633 ,  0.09276772,  1.0700531 ,  1.0378113 ,\n",
       "          0.19040966, -0.6539521 ,  0.6521683 ,  1.3432503 ,\n",
       "         -0.77749014, -1.3917594 , -0.2886529 , -1.6031384 ,\n",
       "         -1.5589604 , -1.5694084 ,  0.4471116 ,  0.4374156 ],\n",
       "        [ 0.06045103, -0.03575808, -0.04571903,  0.03819013,\n",
       "         -0.0296948 , -0.05819136,  0.0837577 , -0.08739978,\n",
       "         -0.04204625,  0.07687181,  0.1402623 , -0.00813192,\n",
       "          0.08044505, -0.0508886 ,  0.14069867,  0.04561377,\n",
       "         -0.0750801 ,  0.00925988,  0.11594802,  0.11121011,\n",
       "          0.01987809, -0.06726319,  0.05808568,  0.12994218,\n",
       "         -0.08304   , -0.13727432, -0.03000951, -0.14642328,\n",
       "         -0.1558466 , -0.14252228,  0.04035962,  0.04431599],\n",
       "        [ 0.48789406, -0.2845416 , -0.35409546,  0.31563425,\n",
       "         -0.2386198 , -0.45782375,  0.7014699 , -0.7089815 ,\n",
       "         -0.32338524,  0.6365533 ,  1.1070447 , -0.06096935,\n",
       "          0.6197591 , -0.40458488,  1.1562061 ,  0.35577202,\n",
       "         -0.6074605 ,  0.07346296,  0.88520145,  0.8533859 ,\n",
       "          0.15439606, -0.52607346,  0.48729753,  1.0464439 ,\n",
       "         -0.6381912 , -1.0951853 , -0.23353052, -1.2129827 ,\n",
       "         -1.2354093 , -1.1840763 ,  0.33630514,  0.34908915]]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opa = Drop_GLU_Add_Norm(units=32,drop_rate=0.0)\n",
    "opa(testando[:,:,1],testando[:,:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##########################################################################\n",
    "class VariableSelectionNetwork(tf.keras.layers.Layer):\n",
    "  def __init__(self, d_model, dropout_rate, additional_context=None, **kwargs):\n",
    "    super(VariableSelectionNetwork, self).__init__(**kwargs)\n",
    "    self.d_model = d_model\n",
    "    self.dropout_rate = dropout_rate\n",
    "    self.additional_context = additional_context\n",
    "\n",
    "    self.flattened_grn = GRN(d_model, drop_rate=dropout_rate, optional_context=additional_context)\n",
    "    self.per_feature_grn = [GRN(d_model, drop_rate=dropout_rate) for _ in range(d_model)]\n",
    "\n",
    "  def call(self, inputs):\n",
    "    # Non-static inputs\n",
    "    if self.additional_context is not None:\n",
    "      embedding, static_context = inputs\n",
    "      time_steps = tf.shape(embedding)[1]\n",
    "\n",
    "      flatten = tf.reshape(embedding, (-1, time_steps, self.d_model * self.output_size))\n",
    "      static_context = tf.expand_dims(static_context, axis=1)\n",
    "\n",
    "      # Nonlinear transformation with gated residual network\n",
    "      mlp_outputs = self.flattened_grn((flatten, static_context))\n",
    "      sparse_weights = tf.nn.softmax(mlp_outputs, axis=-1)\n",
    "      sparse_weights = tf.expand_dims(sparse_weights, axis=2)\n",
    "\n",
    "      trans_emb_list = []\n",
    "      for i in range(self.output_size):\n",
    "        e = self.per_feature_grn[i](embedding[..., i])  # Select i-th feature\n",
    "        trans_emb_list.append(e)\n",
    "      transformed_embedding = tf.concat(trans_emb_list, axis=-1)\n",
    "\n",
    "      combined = sparse_weights * transformed_embedding\n",
    "      temporal_ctx = tf.reduce_sum(combined, axis=-1)\n",
    "\n",
    "    # Static inputs\n",
    "    else:\n",
    "      embedding = inputs\n",
    "      print(\"\\n my input:\\n\")\n",
    "      print(embedding)\n",
    "      flatten = tf.reshape(embedding, [-1, self.d_model])  # Assuming batch_size is the first dimension\n",
    "      print(\"\\n my flatten vector:\\n\")\n",
    "      print(flatten)\n",
    "      # Nonlinear transformation with gated residual network\n",
    "      mlp_outputs = self.flattened_grn(flatten)\n",
    "      print(\"\\n apllying GRN on flatten vector:\\n\")\n",
    "      print(mlp_outputs)\n",
    "      sparse_weights = tf.nn.softmax(mlp_outputs, axis=-1)\n",
    "      print(\"\\n apllying softmax:\\n\")\n",
    "      print(sparse_weights)\n",
    "      sparse_weights = tf.expand_dims(sparse_weights, axis=-1)\n",
    "      print(\"\\n expanding my dimensions:\\n\")\n",
    "      print(sparse_weights)\n",
    "      trans_emb_list = []\n",
    "      for i in range(self.output_size):\n",
    "        e = self.per_feature_grn[i](embedding[:, i:i + 1, :])  # Select i-th feature channel\n",
    "        trans_emb_list.append(e)\n",
    "      transformed_embedding = tf.concat(trans_emb_list, axis=1)\n",
    "\n",
    "      combined = sparse_weights * transformed_embedding\n",
    "      temporal_ctx = tf.reduce_sum(combined, axis=1)\n",
    "\n",
    "    return temporal_ctx, sparse_weights"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow-park",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
