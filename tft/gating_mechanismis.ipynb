{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-05 21:45:59.573127: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-06-05 21:45:59.841017: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-05 21:46:00.511778: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying the temporal fusion transformers gatting mechanismis using tensorflow \n",
    "\n",
    "The teoric text is in my final course work \"\", in 1.6.1 cap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GLU(tf.keras.layers.Layer):\n",
    "    def __init__(self,units):\n",
    "        \"\"\"Define a Gate Linear Unit\n",
    "\n",
    "        Args:\n",
    "            units (int): the units quantities in the dense layer on GLU\n",
    "        \"\"\"\n",
    "        super(GLU,self).__init__()\n",
    "\n",
    "        self.linear = tf.keras.layers.Dense(units)                      #creating a layer to calculate W_{5}x+b_{5}\n",
    "        self.dense_sigmoid = tf.keras.Dense(units,activation='sigmoid') #creating a layer to calculate sigmoid(W_{4}x+b_{4})\n",
    "\n",
    "        def call(self,inputs):\n",
    "            \"\"\"Calculate the element wise multiplication between the layers\n",
    "\n",
    "            Args:\n",
    "                inputs (tf.Tensor,tf.Dataset or np.array): the input to calculate the GLU\n",
    "\n",
    "            Returns:\n",
    "                tf.Tensor,tf.Dataset or np.array: element_wise_product\n",
    "            \"\"\"\n",
    "            element_wise_product = self.dense_sigmoid(inputs) * self.linear(inputs)\n",
    "            return element_wise_product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Drop_GLU_Add_Norm(tf.keras.layers.Layer):\n",
    "    def __init__(self, units, drop_rate):\n",
    "        \"\"\"Define the process of apply a dropout, a GLU add the residuals to GLU output and normalize them\n",
    "\n",
    "        Args:\n",
    "            units (int): the units quantities in the dense layer on GLU\n",
    "            drop_rate (float): a float number 0 <= drop_rate <= 1, where define te dropout rate\n",
    "        \"\"\"\n",
    "        super(Drop_GLU_Add_Norm,self).__init__()\n",
    "        self.units= units\n",
    "        self.drop_rate = drop_rate\n",
    "\n",
    "        self.dropout_layer = tf.keras.layers.Dropout(self.drop_rate)\n",
    "        self.layer_GLU = GLU(self.units)\n",
    "        self.norm_layer = tf.keras.layers.LayerNormalization()\n",
    "\n",
    "    def call(self,inputs, residual):\n",
    "        \"\"\"Compute all process \n",
    "\n",
    "        Args:\n",
    "            inputs (tf.Tensor,tf.Dataset or np.array): real input\n",
    "            residual (tf.Tensor,tf.Dataset or np.array): the residuals input to add to normalize\n",
    "        Returns:\n",
    "            tf.Tensor,tf.Dataset or np.array : normalized_values\n",
    "        \"\"\"\n",
    "        input_droped = self.dropout_layer(inputs)\n",
    "        glu_output = self.layer_GLU(input_droped)\n",
    "        normalized_values = self.norm_layer(glu_output + residual)\n",
    "        return normalized_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRN(tf.keras.layers.Layer):\n",
    "    def __init__(self, units, drop_rate, optional_context=False):\n",
    "        super(GRN,self).__init__()\n",
    "\n",
    "        self.units = units\n",
    "        self.drop_rate = drop_rate\n",
    "        self.optional_context = optional_context\n",
    "\n",
    "        self.layer_ELU = tf.keras.layers.ELU()\n",
    "        self.first_linear = tf.keras.layers.Dense(self.units)\n",
    "        self.second_linear = tf.keras.layers.Dense(self.units)\n",
    "\n",
    "        if self.optional_context:\n",
    "            self.linear_optioal = tf.keras.layers.Dense(self.units,use_bias=False)\n",
    "\n",
    "        \n",
    "        self.add_norm = Drop_GLU_Add_Norm(units=self.units,\n",
    "                                          drop_rate=self.drop_rate)\n",
    "        \n",
    "    def call(self,inputs):\n",
    "        if self.optional_context:\n",
    "            X, context = inputs\n",
    "            dense_out = self.first_linear(X)\n",
    "            context_out = self.linear_optioal(context)\n",
    "            first_output = self.layer_ELU(dense_out + context_out)\n",
    "        else:\n",
    "            X = inputs\n",
    "            dense_out = self.first_linear(X)\n",
    "            first_output = self.layer_ELU(dense_out)\n",
    "\n",
    "        second_output = self.second_linear(first_output)\n",
    "\n",
    "        final_output = self.add_norm(second_output,X)\n",
    "\n",
    "        return final_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow-park",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
